habanalabs driver is not loaded or no AIPs available, aborting...
Obtaining taming-transformers from git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers (from -r requirements.txt (line 16))
  Updating ./src/taming-transformers clone (to revision master)
  Running command git fetch -q --tags
  Running command git reset --hard -q 3ba01b241669f5ade541ce990f7650a3b8f65318
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Obtaining clip from git+https://github.com/openai/CLIP.git@main#egg=clip (from -r requirements.txt (line 17))
  Updating ./src/clip clone (to revision main)
  Running command git fetch -q --tags
  Running command git reset --hard -q a1d071733d7111c9c014f024669f959182114e33
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Obtaining file:///home/models/Model-References/PyTorch/generative_models/stable-diffusion (from -r requirements.txt (line 18))
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting albumentations==0.4.3
  Downloading albumentations-0.4.3.tar.gz (3.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 16.4 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting opencv-python
  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.7/61.7 MB 220.4 MB/s eta 0:00:00
Collecting pudb==2019.2
  Downloading pudb-2019.2.tar.gz (59 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.5/59.5 kB 141.5 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting imageio==2.9.0
  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.3/3.3 MB 302.0 MB/s eta 0:00:00
Collecting imageio-ffmpeg==0.4.2
  Downloading imageio_ffmpeg-0.4.2-py3-none-manylinux2010_x86_64.whl (26.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.9/26.9 MB 211.8 MB/s eta 0:00:00
Collecting lightning==2.0.0
  Downloading lightning-2.0.0-py3-none-any.whl (1.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 287.7 MB/s eta 0:00:00
Collecting torchmetrics==0.10.3
  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 529.7/529.7 kB 435.8 MB/s eta 0:00:00
Collecting omegaconf==2.1.1
  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 74.7/74.7 kB 332.1 MB/s eta 0:00:00
Collecting test-tube>=0.7.5
  Downloading test_tube-0.7.5.tar.gz (21 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting streamlit>=0.73.1
  Downloading streamlit-1.28.2-py2.py3-none-any.whl (8.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.4/8.4 MB 243.4 MB/s eta 0:00:00
Collecting einops==0.3.0
  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)
Collecting torch-fidelity==0.3.0
  Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)
Collecting transformers==4.23.0
  Downloading transformers-4.23.0-py3-none-any.whl (5.3 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 189.4 MB/s eta 0:00:00
Collecting kornia==0.6
  Downloading kornia-0.6.0-py2.py3-none-any.whl (367 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 367.1/367.1 kB 158.1 MB/s eta 0:00:00
Collecting webdataset==0.2.5
  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.9/46.9 kB 204.4 MB/s eta 0:00:00
Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.3->-r requirements.txt (line 1)) (1.23.5)
Collecting scipy
  Downloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 36.4/36.4 MB 213.5 MB/s eta 0:00:00
Collecting imgaug<0.2.7,>=0.2.5
  Downloading imgaug-0.2.6.tar.gz (631 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 631.4/631.4 kB 365.6 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.3->-r requirements.txt (line 1)) (6.0)
Collecting opencv-python-headless>=4.1.1
  Downloading opencv_python_headless-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.1/49.1 MB 265.0 MB/s eta 0:00:00
Collecting urwid>=1.1.1
  Downloading urwid-2.2.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 274.4/274.4 kB 480.1 MB/s eta 0:00:00
Requirement already satisfied: pygments>=1.0 in /usr/local/lib/python3.10/dist-packages (from pudb==2019.2->-r requirements.txt (line 3)) (2.16.1)
Collecting pillow
  Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 366.1 MB/s eta 0:00:00
Requirement already satisfied: Jinja2<5.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (3.1.2)
Requirement already satisfied: websockets<12.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (11.0.3)
Requirement already satisfied: croniter<1.4.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (1.3.15)
Requirement already satisfied: requests<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (2.31.0)
Requirement already satisfied: packaging<25.0,>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (23.1)
Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (13.5.3)
Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (2.0.9)
Requirement already satisfied: lightning-cloud>=0.5.31 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (0.5.38)
Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (4.12.2)
Requirement already satisfied: torch<4.0,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (2.0.1a0+gitdefeb45)
Requirement already satisfied: inquirer<5.0,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (3.1.3)
Requirement already satisfied: starsessions<2.0,>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (1.3.0)
Requirement already satisfied: dateutils<2.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (0.6.12)
Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (5.10.0)
Requirement already satisfied: uvicorn<2.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (0.23.2)
Requirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (2023.9.2)
Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (8.1.7)
Requirement already satisfied: urllib3<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (1.26.16)
Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (4.8.0)
Requirement already satisfied: deepdiff<8.0,>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (6.5.0)
Requirement already satisfied: pydantic<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (2.4.0)
Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (4.66.1)
Collecting fastapi<0.89.0
  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.5/55.5 kB 294.5 MB/s eta 0:00:00
Requirement already satisfied: starlette<2.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (0.27.0)
Requirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (1.6.3)
Requirement already satisfied: arrow<3.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (1.2.3)
Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (5.9.5)
Requirement already satisfied: lightning-utilities<2.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (0.9.0)
Collecting antlr4-python3-runtime==4.8
  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 112.4/112.4 kB 309.4 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from torch-fidelity==0.3.0->-r requirements.txt (line 12)) (0.15.1a0+42759b1)
Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.0->-r requirements.txt (line 13)) (3.12.4)
Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.0->-r requirements.txt (line 13)) (2023.5.5)
Collecting huggingface-hub<1.0,>=0.10.0
  Downloading huggingface_hub-0.19.1-py3-none-any.whl (311 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 311.1/311.1 kB 365.2 MB/s eta 0:00:00
Collecting tokenizers!=0.11.3,<0.14,>=0.11.1
  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 322.4 MB/s eta 0:00:00
Collecting braceexpand
  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)
Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from test-tube>=0.7.5->-r requirements.txt (line 9)) (2.0.1)
Requirement already satisfied: tensorboard>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from test-tube>=0.7.5->-r requirements.txt (line 9)) (2.11.2)
Collecting future
  Downloading future-0.18.3.tar.gz (840 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 840.9/840.9 kB 277.8 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting tenacity<9,>=8.1.0
  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)
Collecting tzlocal<6,>=1.1
  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)
Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 10)) (2.8.2)
Collecting toml<2,>=0.10.1
  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)
Collecting gitpython!=3.1.19,<4,>=3.0.7
  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.6/190.6 kB 237.7 MB/s eta 0:00:00
Collecting validators<1,>=0.2
  Downloading validators-0.22.0-py3-none-any.whl (26 kB)
Collecting pyarrow>=6.0
  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.0/38.0 MB 268.6 MB/s eta 0:00:00
Collecting tornado<7,>=6.0.3
  Downloading tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 427.7/427.7 kB 158.6 MB/s eta 0:00:00
Collecting altair<6,>=4.0
  Downloading altair-5.1.2-py3-none-any.whl (516 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 516.2/516.2 kB 154.0 MB/s eta 0:00:00
Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 10)) (5.3.1)
Collecting blinker<2,>=1.0.0
  Downloading blinker-1.7.0-py3-none-any.whl (13 kB)
Collecting pydeck<1,>=0.8.0b4
  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 280.8 MB/s eta 0:00:00
Collecting importlib-metadata<7,>=1.4
  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)
Collecting watchdog>=2.1.5
  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.1/82.1 kB 227.9 MB/s eta 0:00:00
Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 10)) (3.20.3)
Collecting ftfy
  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.1/53.1 kB 219.8 MB/s eta 0:00:00
Collecting jsonschema>=3.0
  Downloading jsonschema-4.19.2-py3-none-any.whl (83 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 83.6/83.6 kB 240.6 MB/s eta 0:00:00
Collecting toolz
  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.8/55.8 kB 222.4 MB/s eta 0:00:00
Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning==2.0.0->-r requirements.txt (line 6)) (2.5)
Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateutils<2.0->lightning==2.0.0->-r requirements.txt (line 6)) (2023.3.post1)
Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from deepdiff<8.0,>=5.7.0->lightning==2.0.0->-r requirements.txt (line 6)) (4.1.0)
Collecting pydantic<3.0
  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 332.9 MB/s eta 0:00:00
Collecting starlette<2.0
  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.3/64.3 kB 318.7 MB/s eta 0:00:00
Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<2.0->lightning==2.0.0->-r requirements.txt (line 6)) (3.7.1)
Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (3.8.5)
Collecting gitdb<5,>=4.0.1
  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 307.2 MB/s eta 0:00:00
Collecting scikit-image>=0.11.0
  Downloading scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.7/14.7 MB 201.0 MB/s eta 0:00:00
Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.3->-r requirements.txt (line 1)) (1.16.0)
Collecting zipp>=0.5
  Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)
Requirement already satisfied: python-editor>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from inquirer<5.0,>=2.10.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.0.4)
Requirement already satisfied: readchar>=3.0.6 in /usr/local/lib/python3.10/dist-packages (from inquirer<5.0,>=2.10.0->lightning==2.0.0->-r requirements.txt (line 6)) (4.0.5)
Requirement already satisfied: blessed>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from inquirer<5.0,>=2.10.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.20.0)
Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<5.0->lightning==2.0.0->-r requirements.txt (line 6)) (2.1.3)
Requirement already satisfied: pyjwt in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.31->lightning==2.0.0->-r requirements.txt (line 6)) (2.8.0)
Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.31->lightning==2.0.0->-r requirements.txt (line 6)) (0.0.6)
Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.3->test-tube>=0.7.5->-r requirements.txt (line 9)) (2023.3)
Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning==2.0.0->-r requirements.txt (line 6)) (3.4)
Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning==2.0.0->-r requirements.txt (line 6)) (2023.7.22)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning==2.0.0->-r requirements.txt (line 6)) (3.2.0)
Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning==2.0.0->-r requirements.txt (line 6)) (3.0.0)
Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning==2.0.0->-r requirements.txt (line 6)) (2.1.2)
Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (1.8.1)
Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (2.23.0)
Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (1.58.0)
Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (3.4.4)
Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (2.0.0)
Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (2.3.7)
Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (0.41.2)
Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (0.6.1)
Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (68.2.2)
Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (0.4.6)
Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning==2.0.0->-r requirements.txt (line 6)) (3.1)
Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.12)
Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn<2.0->lightning==2.0.0->-r requirements.txt (line 6)) (0.14.0)
Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip->-r requirements.txt (line 17)) (0.2.6)
Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (4.0.3)
Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.9.2)
Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.4.0)
Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (23.1.0)
Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (6.0.4)
Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.3.1)
Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<2.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.3.0)
Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<2.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.1.3)
Collecting smmap<6,>=3.0.1
  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)
Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (0.3.0)
Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (4.9)
Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (1.3.1)
Collecting jsonschema-specifications>=2023.03.6
  Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)
Collecting rpds-py>=0.7.1
  Downloading rpds_py-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 172.0 MB/s eta 0:00:00
Collecting referencing>=0.28.4
  Downloading referencing-0.30.2-py3-none-any.whl (25 kB)
Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning==2.0.0->-r requirements.txt (line 6)) (0.1.2)
Collecting tifffile>=2022.8.12
  Downloading tifffile-2023.9.26-py3-none-any.whl (222 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 222.9/222.9 kB 511.7 MB/s eta 0:00:00
Collecting lazy_loader>=0.3
  Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)
Collecting scikit-image>=0.11.0
  Downloading scikit_image-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 196.5 MB/s eta 0:00:00
Collecting PyWavelets>=1.1.1
  Downloading PyWavelets-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 212.7 MB/s eta 0:00:00
Collecting scikit-image>=0.11.0
  Downloading scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.2/13.2 MB 207.7 MB/s eta 0:00:00
Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.11.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.3.0)
Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (0.5.0)
Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (3.2.2)
Building wheels for collected packages: albumentations, pudb, antlr4-python3-runtime, test-tube, imgaug, future
  Building wheel for albumentations (setup.py): started
  Building wheel for albumentations (setup.py): finished with status 'done'
  Created wheel for albumentations: filename=albumentations-0.4.3-py3-none-any.whl size=60763 sha256=e5080f59ffbaab54147713cc14dfe1056efa31201adc1d380e7c7be8263553c8
  Stored in directory: /tmp/pip-ephem-wheel-cache-oknp83jj/wheels/6d/15/9a/e9e1ded8efc6c809dc42e97dfc5bb4f57267dc02f6a4617f0e
  Building wheel for pudb (setup.py): started
  Building wheel for pudb (setup.py): finished with status 'done'
  Created wheel for pudb: filename=pudb-2019.2-py3-none-any.whl size=63214 sha256=5a30563c4daac412a01558c3fa54f49a06c1843dcc9238d155a586657b854315
  Stored in directory: /tmp/pip-ephem-wheel-cache-oknp83jj/wheels/02/0f/24/1ca86e678056ad3c72b3fe94f829b9bdf92bb4d661c32bbed1
  Building wheel for antlr4-python3-runtime (setup.py): started
  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'
  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141209 sha256=61bbea94d77e62dac0e21e0cee2d15d493c70b0e60b7cc8f2162723d6db857c8
  Stored in directory: /tmp/pip-ephem-wheel-cache-oknp83jj/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5
  Building wheel for test-tube (setup.py): started
  Building wheel for test-tube (setup.py): finished with status 'done'
  Created wheel for test-tube: filename=test_tube-0.7.5-py3-none-any.whl size=25327 sha256=23592ca6b7dff3e0270c4ec233a55217711e5b8e737ac81a683093513ef90c2f
  Stored in directory: /tmp/pip-ephem-wheel-cache-oknp83jj/wheels/28/d4/8b/1aeb47c0dedd931b8e6aec55a8091864a69ac6f0adc5b12ea9
  Building wheel for imgaug (setup.py): started
  Building wheel for imgaug (setup.py): finished with status 'done'
  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654002 sha256=b8d73776dec0bf6c605b554e4dae92463d2ac7dd59122723b2d9b8cd6ca1f989
  Stored in directory: /tmp/pip-ephem-wheel-cache-oknp83jj/wheels/cb/c7/a6/2d7a113c4885dc0f4eacd8f41095763181c0b9a18223ac7533
  Building wheel for future (setup.py): started
  Building wheel for future (setup.py): finished with status 'done'
  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492024 sha256=ce3de08f7fd45a65bdb3a6f347179599e43ee36f16bde1833864cda33b7e31e3
  Stored in directory: /tmp/pip-ephem-wheel-cache-oknp83jj/wheels/5e/a9/47/f118e66afd12240e4662752cc22cefae5d97275623aa8ef57d
Successfully built albumentations pudb antlr4-python3-runtime test-tube imgaug future
Installing collected packages: tokenizers, einops, braceexpand, antlr4-python3-runtime, zipp, webdataset, watchdog, validators, urwid, tzlocal, tornado, toolz, toml, tifffile, tenacity, smmap, scipy, rpds-py, PyWavelets, pydantic, pyarrow, pillow, opencv-python-headless, opencv-python, omegaconf, lazy_loader, imageio-ffmpeg, future, ftfy, blinker, starlette, referencing, pydeck, pudb, importlib-metadata, imageio, huggingface-hub, gitdb, transformers, torchmetrics, taming-transformers, scikit-image, latent-diffusion, kornia, jsonschema-specifications, gitpython, fastapi, torch-fidelity, jsonschema, imgaug, clip, test-tube, lightning, altair, albumentations, streamlit
  Attempting uninstall: pydantic
    Found existing installation: pydantic 2.4.0
    Uninstalling pydantic-2.4.0:
      Successfully uninstalled pydantic-2.4.0
  Attempting uninstall: starlette
    Found existing installation: starlette 0.27.0
    Uninstalling starlette-0.27.0:
      Successfully uninstalled starlette-0.27.0
  Attempting uninstall: torchmetrics
    Found existing installation: torchmetrics 1.2.0
    Uninstalling torchmetrics-1.2.0:
      Successfully uninstalled torchmetrics-1.2.0
  Running setup.py develop for taming-transformers
  Running setup.py develop for latent-diffusion
  Attempting uninstall: fastapi
    Found existing installation: fastapi 0.103.1
    Uninstalling fastapi-0.103.1:
      Successfully uninstalled fastapi-0.103.1
  Running setup.py develop for clip
  Attempting uninstall: lightning
    Found existing installation: lightning 2.0.4
    Uninstalling lightning-2.0.4:
      Successfully uninstalled lightning-2.0.4
Successfully installed PyWavelets-1.4.1 albumentations-0.4.3 altair-5.1.2 antlr4-python3-runtime-4.8 blinker-1.7.0 braceexpand-0.1.7 clip-1.0 einops-0.3.0 fastapi-0.88.0 ftfy-6.1.1 future-0.18.3 gitdb-4.0.11 gitpython-3.1.40 huggingface-hub-0.19.1 imageio-2.9.0 imageio-ffmpeg-0.4.2 imgaug-0.2.6 importlib-metadata-6.8.0 jsonschema-4.19.2 jsonschema-specifications-2023.7.1 kornia-0.6.0 latent-diffusion-0.0.1 lazy_loader-0.3 lightning-2.0.0 omegaconf-2.1.1 opencv-python-4.8.1.78 opencv-python-headless-4.8.1.78 pillow-10.1.0 pudb-2019.2 pyarrow-14.0.1 pydantic-1.10.13 pydeck-0.8.1b0 referencing-0.30.2 rpds-py-0.12.0 scikit-image-0.20.0 scipy-1.11.3 smmap-5.0.1 starlette-0.22.0 streamlit-1.28.2 taming-transformers-0.0.1 tenacity-8.2.3 test-tube-0.7.5 tifffile-2023.9.26 tokenizers-0.13.3 toml-0.10.2 toolz-0.12.0 torch-fidelity-0.3.0 torchmetrics-0.10.3 tornado-6.3.3 transformers-4.23.0 tzlocal-5.2 urwid-2.2.3 validators-0.22.0 watchdog-3.0.0 webdataset-0.2.5 zipp-3.17.0
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
/root:/usr/lib/habanalabs/:/home/models/Model-References:/home/models/Model-References/Pytorch/generative_models/stable-diffusion/src/taming-transformers:/usr/lib/habanalabs
hi
/home/models/Model-References/PyTorch/generative_models/stable-diffusion
/scratch/tmp
Warning: Permanently added 'javierhn-stable-8cards-worker-0.javierhn-stable-8cards-worker.default.svc' (ED25519) to the list of known hosts.
 * Starting OpenBSD Secure Shell server sshd
   ...done.
[1,0]<stdout>:Obtaining taming-transformers from git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers (from -r requirements.txt (line 16))
[1,0]<stdout>:  Updating ./src/taming-transformers clone (to revision master)
[1,0]<stderr>:  Running command git fetch -q --tags
[1,0]<stderr>:  Running command git reset --hard -q 3ba01b241669f5ade541ce990f7650a3b8f65318
[1,0]<stdout>:  Preparing metadata (setup.py) ... [?25l-[1,0]<stdout>: done
[1,0]<stdout>:[?25hObtaining clip from git+https://github.com/openai/CLIP.git@main#egg=clip (from -r requirements.txt (line 17))
[1,0]<stdout>:  Updating ./src/clip clone (to revision main)
[1,0]<stderr>:  Running command git fetch -q --tags
[1,0]<stderr>:  Running command git reset --hard -q a1d071733d7111c9c014f024669f959182114e33
[1,0]<stdout>:  Preparing metadata (setup.py) ... [?25l-[1,0]<stdout>: \[1,0]<stdout>: done
[1,0]<stdout>:[?25hObtaining file:///home/models/Model-References/PyTorch/generative_models/stable-diffusion (from -r requirements.txt (line 18))
[1,0]<stdout>:  Preparing metadata (setup.py) ... [?25l-[1,0]<stdout>: done
[1,0]<stdout>:[?25hCollecting albumentations==0.4.3
[1,0]<stdout>:  Downloading albumentations-0.4.3.tar.gz (3.2 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/3.2 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.1/3.2 MB[0m [31m3.1 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.4/3.2 MB[0m [31m6.4 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.9/3.2 MB[0m [31m8.4 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━[0m [32m1.5/3.2 MB[0m [31m10.6 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━[0m [32m2.4/3.2 MB[0m [31m13.4 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m3.2/3.2 MB[0m [31m15.5 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.2/3.2 MB[0m [31m14.4 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:  Preparing metadata (setup.py) ... [?25l-[1,0]<stdout>: done
[1,0]<stdout>:[?25hCollecting opencv-python
[1,0]<stdout>:  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/61.7 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.5/61.7 MB[0m [31m45.2 MB/s[0m eta [36m0:00:02[0m[1,0]<stdout>:[2K     [91m━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.6/61.7 MB[0m [31m52.1 MB/s[0m eta [36m0:00:02[0m[1,0]<stdout>:[2K     [91m━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m6.6/61.7 MB[0m [31m62.1 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m10.7/61.7 MB[0m [31m77.5 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m15.7/61.7 MB[0m [31m123.5 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m24.0/61.7 MB[0m [31m208.2 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m32.5/61.7 MB[0m [31m243.3 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━[0m [32m41.2/61.7 MB[0m [31m246.2 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━[0m [32m50.1/61.7 MB[0m [31m250.9 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━[0m [32m57.5/61.7 MB[0m [31m223.7 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m61.7/61.7 MB[0m [31m216.6 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m61.7/61.7 MB[0m [31m216.6 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m61.7/61.7 MB[0m [31m216.6 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m61.7/61.7 MB[0m [31m216.6 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m61.7/61.7 MB[0m [31m216.6 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m61.7/61.7 MB[0m [31m38.8 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting pudb==2019.2
[1,0]<stdout>:  Downloading pudb-2019.2.tar.gz (59 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/59.5 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m59.5/59.5 kB[0m [31m14.4 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:  Preparing metadata (setup.py) ... [?25l-[1,0]<stdout>: done
[1,0]<stdout>:[?25hCollecting imageio==2.9.0
[1,0]<stdout>:  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/3.3 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.3/3.3 MB[0m [31m123.7 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting imageio-ffmpeg==0.4.2
[1,0]<stdout>:  Downloading imageio_ffmpeg-0.4.2-py3-none-manylinux2010_x86_64.whl (26.9 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/26.9 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m7.5/26.9 MB[0m [31m224.8 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━[0m [32m16.0/26.9 MB[0m [31m226.1 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━[0m [32m20.2/26.9 MB[0m [31m172.3 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m26.9/26.9 MB[0m [31m175.7 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m26.9/26.9 MB[0m [31m175.7 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m26.9/26.9 MB[0m [31m65.0 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting lightning==2.0.0
[1,0]<stdout>:  Downloading lightning-2.0.0-py3-none-any.whl (1.8 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/1.8 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.8/1.8 MB[0m [31m121.3 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting torchmetrics==0.10.3
[1,0]<stdout>:  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/529.7 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m529.7/529.7 kB[0m [31m45.9 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting omegaconf==2.1.1
[1,0]<stdout>:  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/74.7 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m74.7/74.7 kB[0m [31m14.0 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting test-tube>=0.7.5
[1,0]<stdout>:  Downloading test_tube-0.7.5.tar.gz (21 kB)
[1,0]<stdout>:  Preparing metadata (setup.py) ... [?25l-[1,0]<stdout>: done
[1,0]<stdout>:[?25hCollecting streamlit>=0.73.1
[1,0]<stdout>:  Downloading streamlit-1.28.2-py2.py3-none-any.whl (8.4 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/8.4 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━[0m [32m7.4/8.4 MB[0m [31m221.5 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m8.4/8.4 MB[0m [31m125.4 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting einops==0.3.0
[1,0]<stdout>:  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)
[1,0]<stdout>:Collecting torch-fidelity==0.3.0
[1,0]<stdout>:  Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)
[1,0]<stdout>:Collecting transformers==4.23.0
[1,0]<stdout>:  Downloading transformers-4.23.0-py3-none-any.whl (5.3 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/5.3 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m5.3/5.3 MB[0m [31m253.3 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m5.3/5.3 MB[0m [31m150.3 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting kornia==0.6
[1,0]<stdout>:  Downloading kornia-0.6.0-py2.py3-none-any.whl (367 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/367.1 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m367.1/367.1 kB[0m [31m44.4 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting webdataset==0.2.5
[1,0]<stdout>:  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/46.9 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m46.9/46.9 kB[0m [31m10.5 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.3->-r requirements.txt (line 1)) (1.23.5)
[1,0]<stdout>:Collecting scipy
[1,0]<stdout>:  Downloading scipy-1.11.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/36.4 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m8.4/36.4 MB[0m [31m253.4 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━[0m [32m17.3/36.4 MB[0m [31m254.1 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━[0m [32m26.2/36.4 MB[0m [31m253.6 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━[0m [32m35.1/36.4 MB[0m [31m253.6 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m36.4/36.4 MB[0m [31m243.3 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m36.4/36.4 MB[0m [31m243.3 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m36.4/36.4 MB[0m [31m243.3 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m36.4/36.4 MB[0m [31m61.3 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting imgaug<0.2.7,>=0.2.5
[1,0]<stdout>:  Downloading imgaug-0.2.6.tar.gz (631 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/631.4 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m631.4/631.4 kB[0m [31m82.3 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:  Preparing metadata (setup.py) ... [?25l-[1,0]<stdout>: done
[1,0]<stdout>:[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations==0.4.3->-r requirements.txt (line 1)) (6.0)
[1,0]<stdout>:Collecting opencv-python-headless>=4.1.1
[1,0]<stdout>:  Downloading opencv_python_headless-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/49.1 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m8.5/49.1 MB[0m [31m256.2 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m17.7/49.1 MB[0m [31m246.5 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━[0m [32m26.2/49.1 MB[0m [31m258.2 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━[0m [32m31.1/49.1 MB[0m [31m179.9 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━[0m [32m40.2/49.1 MB[0m [31m182.5 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m49.1/49.1 MB[0m [31m256.2 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m49.1/49.1 MB[0m [31m256.2 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m49.1/49.1 MB[0m [31m256.2 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m49.1/49.1 MB[0m [31m256.2 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m49.1/49.1 MB[0m [31m48.9 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting urwid>=1.1.1
[1,0]<stdout>:  Downloading urwid-2.2.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/274.4 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m274.4/274.4 kB[0m [31m42.5 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Requirement already satisfied: pygments>=1.0 in /usr/local/lib/python3.10/dist-packages (from pudb==2019.2->-r requirements.txt (line 3)) (2.16.1)
[1,0]<stdout>:Collecting pillow
[1,0]<stdout>:  Downloading Pillow-10.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.6 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/3.6 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.6/3.6 MB[0m [31m142.8 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (4.66.1)
[1,0]<stdout>:Requirement already satisfied: deepdiff<8.0,>=5.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (6.5.0)
[1,0]<stdout>:Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (23.1)
[1,0]<stdout>:Requirement already satisfied: torch<4.0,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (2.0.1a0+gitdefeb45)
[1,0]<stdout>:Requirement already satisfied: websockets<12.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (11.0.3)
[1,0]<stdout>:Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (13.5.3)
[1,0]<stdout>:Requirement already satisfied: lightning-utilities<2.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (0.9.0)
[1,0]<stdout>:Requirement already satisfied: requests<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (2.31.0)
[1,0]<stdout>:Requirement already satisfied: starsessions<2.0,>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (1.3.0)
[1,0]<stdout>:Collecting fastapi<0.89.0
[1,0]<stdout>:  Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/55.5 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m55.5/55.5 kB[0m [31m11.5 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Requirement already satisfied: Jinja2<5.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (3.1.2)
[1,0]<stdout>:Requirement already satisfied: dateutils<2.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (0.6.12)
[1,0]<stdout>:Requirement already satisfied: croniter<1.4.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (1.3.15)
[1,0]<stdout>:Requirement already satisfied: lightning-cloud>=0.5.31 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (0.5.38)
[1,0]<stdout>:Requirement already satisfied: uvicorn<2.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (0.23.2)
[1,0]<stdout>:Requirement already satisfied: inquirer<5.0,>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (3.1.3)
[1,0]<stdout>:Requirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (2023.9.2)
[1,0]<stdout>:Requirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (1.6.3)
[1,0]<stdout>:Requirement already satisfied: starlette<2.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (0.27.0)
[1,0]<stdout>:Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (5.10.0)
[1,0]<stdout>:Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (8.1.7)
[1,0]<stdout>:Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (4.8.0)
[1,0]<stdout>:Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (5.9.5)
[1,0]<stdout>:Requirement already satisfied: arrow<3.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (1.2.3)
[1,0]<stdout>:Requirement already satisfied: pydantic<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (2.4.0)
[1,0]<stdout>:Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (2.0.9)
[1,0]<stdout>:Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (4.12.2)
[1,0]<stdout>:Requirement already satisfied: urllib3<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning==2.0.0->-r requirements.txt (line 6)) (1.26.16)
[1,0]<stdout>:Collecting antlr4-python3-runtime==4.8
[1,0]<stdout>:  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/112.4 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m112.4/112.4 kB[0m [31m29.8 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:  Preparing metadata (setup.py) ... [?25l-[1,0]<stdout>: done
[1,0]<stdout>:[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from torch-fidelity==0.3.0->-r requirements.txt (line 12)) (0.15.1a0+42759b1)
[1,0]<stdout>:Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.0->-r requirements.txt (line 13)) (2023.5.5)
[1,0]<stdout>:Collecting tokenizers!=0.11.3,<0.14,>=0.11.1
[1,0]<stdout>:  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/7.8 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━[0m [32m7.3/7.8 MB[0m [31m221.4 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m7.8/7.8 MB[0m [31m140.5 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.23.0->-r requirements.txt (line 13)) (3.12.4)
[1,0]<stdout>:Collecting huggingface-hub<1.0,>=0.10.0
[1,0]<stdout>:  Downloading huggingface_hub-0.19.1-py3-none-any.whl (311 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/311.1 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m311.1/311.1 kB[0m [31m43.0 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting braceexpand
[1,0]<stdout>:  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)
[1,0]<stdout>:Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from test-tube>=0.7.5->-r requirements.txt (line 9)) (2.0.1)
[1,0]<stdout>:Requirement already satisfied: tensorboard>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from test-tube>=0.7.5->-r requirements.txt (line 9)) (2.11.2)
[1,0]<stdout>:Collecting future
[1,0]<stdout>:  Downloading future-0.18.3.tar.gz (840 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/840.9 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m840.9/840.9 kB[0m [31m84.8 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:  Preparing metadata (setup.py) ... [?25l-[1,0]<stdout>: done
[1,0]<stdout>:[?25hCollecting pyarrow>=6.0
[1,0]<stdout>:  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/38.0 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m8.4/38.0 MB[0m [31m252.9 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━━[0m [32m14.7/38.0 MB[0m [31m219.9 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━[0m [32m21.8/38.0 MB[0m [31m188.5 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━[0m [32m30.9/38.0 MB[0m [31m253.4 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m38.0/38.0 MB[0m [31m256.6 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m38.0/38.0 MB[0m [31m256.6 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m38.0/38.0 MB[0m [31m256.6 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m38.0/38.0 MB[0m [31m60.9 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting gitpython!=3.1.19,<4,>=3.0.7
[1,0]<stdout>:  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/190.6 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m190.6/190.6 kB[0m [31m43.3 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Requirement already satisfied: python-dateutil<3,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 10)) (2.8.2)
[1,0]<stdout>:Collecting altair<6,>=4.0
[1,0]<stdout>:  Downloading altair-5.1.2-py3-none-any.whl (516 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/516.2 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m516.2/516.2 kB[0m [31m60.4 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting toml<2,>=0.10.1
[1,0]<stdout>:  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)
[1,0]<stdout>:Collecting tornado<7,>=6.0.3
[1,0]<stdout>:  Downloading tornado-6.3.3-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (427 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/427.7 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m427.7/427.7 kB[0m [31m65.3 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 10)) (3.20.3)
[1,0]<stdout>:Collecting blinker<2,>=1.0.0
[1,0]<stdout>:  Downloading blinker-1.7.0-py3-none-any.whl (13 kB)
[1,0]<stdout>:Collecting pydeck<1,>=0.8.0b4
[1,0]<stdout>:  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/4.8 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m4.8/4.8 MB[0m [31m149.4 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting watchdog>=2.1.5
[1,0]<stdout>:  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/82.1 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m82.1/82.1 kB[0m [31m20.5 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting tenacity<9,>=8.1.0
[1,0]<stdout>:  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)
[1,0]<stdout>:Collecting tzlocal<6,>=1.1
[1,0]<stdout>:  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)
[1,0]<stdout>:Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 10)) (5.3.1)
[1,0]<stdout>:Collecting validators<1,>=0.2
[1,0]<stdout>:  Downloading validators-0.22.0-py3-none-any.whl (26 kB)
[1,0]<stdout>:Collecting importlib-metadata<7,>=1.4
[1,0]<stdout>:  Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB)
[1,0]<stdout>:Collecting ftfy
[1,0]<stdout>:  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/53.1 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m53.1/53.1 kB[0m [31m10.9 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting toolz
[1,0]<stdout>:  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/55.8 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m55.8/55.8 kB[0m [31m14.1 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting jsonschema>=3.0
[1,0]<stdout>:  Downloading jsonschema-4.19.2-py3-none-any.whl (83 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/83.6 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m83.6/83.6 kB[0m [31m22.5 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning==2.0.0->-r requirements.txt (line 6)) (2.5)
[1,0]<stdout>:Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateutils<2.0->lightning==2.0.0->-r requirements.txt (line 6)) (2023.3.post1)
[1,0]<stdout>:Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from deepdiff<8.0,>=5.7.0->lightning==2.0.0->-r requirements.txt (line 6)) (4.1.0)
[1,0]<stdout>:Collecting pydantic<3.0
[1,0]<stdout>:  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/3.1 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m3.1/3.1 MB[0m [31m147.1 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting starlette<2.0
[1,0]<stdout>:  Downloading starlette-0.22.0-py3-none-any.whl (64 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/64.3 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m64.3/64.3 kB[0m [31m15.8 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<2.0->lightning==2.0.0->-r requirements.txt (line 6)) (3.7.1)
[1,0]<stdout>:Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (3.8.5)
[1,0]<stdout>:Collecting gitdb<5,>=4.0.1
[1,0]<stdout>:  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/62.7 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m62.7/62.7 kB[0m [31m18.1 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting scikit-image>=0.11.0
[1,0]<stdout>:  Downloading scikit_image-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.7 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/14.7 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━[0m [32m8.1/14.7 MB[0m [31m245.1 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m14.7/14.7 MB[0m [31m260.2 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m14.7/14.7 MB[0m [31m132.5 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.3->-r requirements.txt (line 1)) (1.16.0)
[1,0]<stdout>:Collecting zipp>=0.5
[1,0]<stdout>:  Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB)
[1,0]<stdout>:Requirement already satisfied: readchar>=3.0.6 in /usr/local/lib/python3.10/dist-packages (from inquirer<5.0,>=2.10.0->lightning==2.0.0->-r requirements.txt (line 6)) (4.0.5)
[1,0]<stdout>:Requirement already satisfied: blessed>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from inquirer<5.0,>=2.10.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.20.0)
[1,0]<stdout>:Requirement already satisfied: python-editor>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from inquirer<5.0,>=2.10.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.0.4)
[1,0]<stdout>:Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<5.0->lightning==2.0.0->-r requirements.txt (line 6)) (2.1.3)
[1,0]<stdout>:Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.31->lightning==2.0.0->-r requirements.txt (line 6)) (0.0.6)
[1,0]<stdout>:Requirement already satisfied: pyjwt in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.31->lightning==2.0.0->-r requirements.txt (line 6)) (2.8.0)
[1,0]<stdout>:Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.20.3->test-tube>=0.7.5->-r requirements.txt (line 9)) (2023.3)
[1,0]<stdout>:Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning==2.0.0->-r requirements.txt (line 6)) (3.4)
[1,0]<stdout>:Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning==2.0.0->-r requirements.txt (line 6)) (3.2.0)
[1,0]<stdout>:Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning==2.0.0->-r requirements.txt (line 6)) (2023.7.22)
[1,0]<stdout>:Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning==2.0.0->-r requirements.txt (line 6)) (3.0.0)
[1,0]<stdout>:Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning==2.0.0->-r requirements.txt (line 6)) (2.1.2)
[1,0]<stdout>:Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (2.23.0)
[1,0]<stdout>:Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (3.4.4)
[1,0]<stdout>:Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (0.41.2)
[1,0]<stdout>:Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (1.58.0)
[1,0]<stdout>:Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (1.8.1)
[1,0]<stdout>:Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (2.0.0)
[1,0]<stdout>:Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (0.6.1)
[1,0]<stdout>:Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (0.4.6)
[1,0]<stdout>:Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (68.2.2)
[1,0]<stdout>:Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (2.3.7)
[1,0]<stdout>:Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning==2.0.0->-r requirements.txt (line 6)) (3.1)
[1,0]<stdout>:Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.12)
[1,0]<stdout>:Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn<2.0->lightning==2.0.0->-r requirements.txt (line 6)) (0.14.0)
[1,0]<stdout>:Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip->-r requirements.txt (line 17)) (0.2.6)
[1,0]<stdout>:Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (4.0.3)
[1,0]<stdout>:Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.9.2)
[1,0]<stdout>:Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (6.0.4)
[1,0]<stdout>:Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (23.1.0)
[1,0]<stdout>:Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.4.0)
[1,0]<stdout>:Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.3.1)
[1,0]<stdout>:Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<2.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.1.3)
[1,0]<stdout>:Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<2.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.3.0)
[1,0]<stdout>:Collecting smmap<6,>=3.0.1
[1,0]<stdout>:  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)
[1,0]<stdout>:Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (4.9)
[1,0]<stdout>:Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (0.3.0)
[1,0]<stdout>:Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (1.3.1)
[1,0]<stdout>:Collecting jsonschema-specifications>=2023.03.6
[1,0]<stdout>:  Downloading jsonschema_specifications-2023.7.1-py3-none-any.whl (17 kB)
[1,0]<stdout>:Collecting rpds-py>=0.7.1
[1,0]<stdout>:  Downloading rpds_py-0.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/1.2 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m1.2/1.2 MB[0m [31m100.8 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting referencing>=0.28.4
[1,0]<stdout>:  Downloading referencing-0.30.2-py3-none-any.whl (25 kB)
[1,0]<stdout>:Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning==2.0.0->-r requirements.txt (line 6)) (0.1.2)
[1,0]<stdout>:Collecting tifffile>=2022.8.12
[1,0]<stdout>:  Downloading tifffile-2023.9.26-py3-none-any.whl (222 kB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/222.9 kB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m222.9/222.9 kB[0m [31m40.7 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting scikit-image>=0.11.0
[1,0]<stdout>:  Downloading scikit_image-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/13.8 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━[0m[90m╺[0m[90m━━━━━━━━━━━━━━━━━━━━━━[0m [32m6.0/13.8 MB[0m [31m181.3 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m13.8/13.8 MB[0m [31m210.1 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m13.8/13.8 MB[0m [31m128.7 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting lazy_loader>=0.2
[1,0]<stdout>:  Downloading lazy_loader-0.3-py3-none-any.whl (9.1 kB)
[1,0]<stdout>:Collecting scikit-image>=0.11.0
[1,0]<stdout>:  Downloading scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/13.2 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m[90m━━━━━━━━━━━━━━━━━━━[0m [32m6.9/13.2 MB[0m [31m207.0 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m13.2/13.2 MB[0m [31m211.6 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m13.2/13.2 MB[0m [31m135.6 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Collecting PyWavelets>=1.1.1
[1,0]<stdout>:  Downloading PyWavelets-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)
[1,0]<stdout>:[?25l[1,0]<stdout>:     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m0.0/6.8 MB[0m [31m?[0m eta [36m-:--:--[0m[1,0]<stdout>:[2K     [91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m[91m╸[0m [32m6.8/6.8 MB[0m [31m224.8 MB/s[0m eta [36m0:00:01[0m[1,0]<stdout>:[2K     [90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━[0m [32m6.8/6.8 MB[0m [31m145.8 MB/s[0m eta [36m0:00:00[0m
[1,0]<stdout>:[?25h[1,0]<stdout>:Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.11.0->lightning==2.0.0->-r requirements.txt (line 6)) (1.3.0)
[1,0]<stdout>:Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (0.5.0)
[1,0]<stdout>:Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15.0->test-tube>=0.7.5->-r requirements.txt (line 9)) (3.2.2)
[1,0]<stdout>:Building wheels for collected packages: albumentations, pudb, antlr4-python3-runtime, test-tube, imgaug, future
[1,0]<stdout>:  Building wheel for albumentations (setup.py) ... [?25l-[1,0]<stdout>: \[1,0]<stdout>: done
[1,0]<stdout>:[?25h  Created wheel for albumentations: filename=albumentations-0.4.3-py3-none-any.whl size=60763 sha256=59e2723e4f9ea9882f8c7196a8a1478966bf517b40dcc9ead3e7c1937e16f3fa
[1,0]<stdout>:  Stored in directory: /root/.cache/pip/wheels/6d/15/9a/e9e1ded8efc6c809dc42e97dfc5bb4f57267dc02f6a4617f0e
[1,0]<stdout>:  Building wheel for pudb (setup.py) ... [?25l-[1,0]<stdout>: \[1,0]<stdout>: done
[1,0]<stdout>:[?25h  Created wheel for pudb: filename=pudb-2019.2-py3-none-any.whl size=63214 sha256=dfc53ac5562ea63da4918b48647132e44fcdd42ba9eb325ae66096ac4bc4e1cc
[1,0]<stdout>:  Stored in directory: /root/.cache/pip/wheels/02/0f/24/1ca86e678056ad3c72b3fe94f829b9bdf92bb4d661c32bbed1
[1,0]<stdout>:  Building wheel for antlr4-python3-runtime (setup.py) ... [?25l-[1,0]<stdout>: \[1,0]<stdout>: done
[1,0]<stdout>:[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141209 sha256=90777e9232577553a05dfc3d67d26873461d5737121b77c804761704bccbb41e
[1,0]<stdout>:  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5
[1,0]<stdout>:  Building wheel for test-tube (setup.py) ... [?25l-[1,0]<stdout>: \[1,0]<stdout>: done
[1,0]<stdout>:[?25h  Created wheel for test-tube: filename=test_tube-0.7.5-py3-none-any.whl size=25327 sha256=7e8a9dccd6e970a25a45b8d830ed78eea142d1d3eb6bfa29d8355d54af5a2fef
[1,0]<stdout>:  Stored in directory: /root/.cache/pip/wheels/28/d4/8b/1aeb47c0dedd931b8e6aec55a8091864a69ac6f0adc5b12ea9
[1,0]<stdout>:  Building wheel for imgaug (setup.py) ... [?25l-[1,0]<stdout>: \[1,0]<stdout>: |[1,0]<stdout>: done
[1,0]<stdout>:[?25h  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654002 sha256=89e345466b87e7b215a6e4c4fd3cad804803c9f53eb87cd3e1243d8ff83284f3
[1,0]<stdout>:  Stored in directory: /root/.cache/pip/wheels/cb/c7/a6/2d7a113c4885dc0f4eacd8f41095763181c0b9a18223ac7533
[1,0]<stdout>:  Building wheel for future (setup.py) ... [?25l-[1,0]<stdout>: \[1,0]<stdout>: |[1,0]<stdout>: /[1,0]<stdout>: done
[1,0]<stdout>:[?25h  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492024 sha256=66645ca15293a84bd3d80d4d77ab2a86a614204676b689415673680515b19f6d
[1,0]<stdout>:  Stored in directory: /root/.cache/pip/wheels/5e/a9/47/f118e66afd12240e4662752cc22cefae5d97275623aa8ef57d
[1,0]<stdout>:Successfully built albumentations pudb antlr4-python3-runtime test-tube imgaug future
[1,0]<stdout>:Installing collected packages: tokenizers, einops, braceexpand, antlr4-python3-runtime, zipp, webdataset, watchdog, validators, urwid, tzlocal, tornado, toolz, toml, tifffile, tenacity, smmap, scipy, rpds-py, PyWavelets, pydantic, pyarrow, pillow, opencv-python-headless, opencv-python, omegaconf, lazy_loader, imageio-ffmpeg, future, ftfy, blinker, starlette, referencing, pydeck, pudb, importlib-metadata, imageio, huggingface-hub, gitdb, transformers, torchmetrics, taming-transformers, scikit-image, latent-diffusion, kornia, jsonschema-specifications, gitpython, fastapi, torch-fidelity, jsonschema, imgaug, clip, test-tube, lightning, altair, albumentations, streamlit
[1,0]<stdout>:  Attempting uninstall: pydantic
[1,0]<stdout>:    Found existing installation: pydantic 2.4.0
[1,0]<stdout>:    Uninstalling pydantic-2.4.0:
[1,0]<stdout>:      Successfully uninstalled pydantic-2.4.0
[1,0]<stdout>:  Attempting uninstall: starlette
[1,0]<stdout>:    Found existing installation: starlette 0.27.0
[1,0]<stdout>:    Uninstalling starlette-0.27.0:
[1,0]<stdout>:      Successfully uninstalled starlette-0.27.0
[1,0]<stdout>:  Attempting uninstall: torchmetrics
[1,0]<stdout>:    Found existing installation: torchmetrics 1.2.0
[1,0]<stdout>:    Uninstalling torchmetrics-1.2.0:
[1,0]<stdout>:      Successfully uninstalled torchmetrics-1.2.0
[1,0]<stdout>:  Running setup.py develop for taming-transformers
[1,0]<stdout>:  Running setup.py develop for latent-diffusion
[1,0]<stdout>:  Attempting uninstall: fastapi
[1,0]<stdout>:    Found existing installation: fastapi 0.103.1
[1,0]<stdout>:    Uninstalling fastapi-0.103.1:
[1,0]<stdout>:      Successfully uninstalled fastapi-0.103.1
[1,0]<stdout>:  Running setup.py develop for clip
[1,0]<stdout>:  Attempting uninstall: lightning
[1,0]<stdout>:    Found existing installation: lightning 2.0.4
[1,0]<stdout>:    Uninstalling lightning-2.0.4:
[1,0]<stdout>:      Successfully uninstalled lightning-2.0.4
[1,0]<stdout>:Successfully installed PyWavelets-1.4.1 albumentations-0.4.3 altair-5.1.2 antlr4-python3-runtime-4.8 blinker-1.7.0 braceexpand-0.1.7 clip-1.0 einops-0.3.0 fastapi-0.88.0 ftfy-6.1.1 future-0.18.3 gitdb-4.0.11 gitpython-3.1.40 huggingface-hub-0.19.1 imageio-2.9.0 imageio-ffmpeg-0.4.2 imgaug-0.2.6 importlib-metadata-6.8.0 jsonschema-4.19.2 jsonschema-specifications-2023.7.1 kornia-0.6.0 latent-diffusion-0.0.1 lazy_loader-0.3 lightning-2.0.0 omegaconf-2.1.1 opencv-python-4.8.1.78 opencv-python-headless-4.8.1.78 pillow-10.1.0 pudb-2019.2 pyarrow-14.0.1 pydantic-1.10.13 pydeck-0.8.1b0 referencing-0.30.2 rpds-py-0.12.0 scikit-image-0.20.0 scipy-1.11.3 smmap-5.0.1 starlette-0.22.0 streamlit-1.28.2 taming-transformers-0.0.1 tenacity-8.2.3 test-tube-0.7.5 tifffile-2023.9.26 tokenizers-0.13.3 toml-0.10.2 toolz-0.12.0 torch-fidelity-0.3.0 torchmetrics-0.10.3 tornado-6.3.3 transformers-4.23.0 tzlocal-5.2 urwid-2.2.3 validators-0.22.0 watchdog-3.0.0 webdataset-0.2.5 zipp-3.17.0
[1,0]<stderr>:WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
[1,0]<stderr>:
[1,0]<stderr>:[notice] A new release of pip available: 22.2.2 -> 23.3.1
[1,0]<stderr>:[notice] To update, run: python3 -m pip install --upgrade pip
Warning: Permanently added 'javierhn-stable-8cards-worker-0.javierhn-stable-8cards-worker.default.svc' (ED25519) to the list of known hosts.
 * Starting OpenBSD Secure Shell server sshd
   ...done.
[1,6]<stderr>:[javierhn-stable-8cards-worker-0:00125] MCW rank 6 bound to socket 1[core 36[hwt 0-1]], socket 1[core 37[hwt 0-1]], socket 1[core 38[hwt 0-1]], socket 1[core 39[hwt 0-1]], socket 1[core 40[hwt 0-1]], socket 1[core 41[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/../../../../../..]
[1,7]<stderr>:[javierhn-stable-8cards-worker-0:00125] MCW rank 7 bound to socket 1[core 42[hwt 0-1]], socket 1[core 43[hwt 0-1]], socket 1[core 44[hwt 0-1]], socket 1[core 45[hwt 0-1]], socket 1[core 46[hwt 0-1]], socket 1[core 47[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB]
[1,0]<stderr>:[javierhn-stable-8cards-worker-0:00125] MCW rank 0 bound to socket 0[core 0[hwt 0-1]], socket 0[core 1[hwt 0-1]], socket 0[core 2[hwt 0-1]], socket 0[core 3[hwt 0-1]], socket 0[core 4[hwt 0-1]], socket 0[core 5[hwt 0-1]]: [BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../..]
[1,1]<stderr>:[javierhn-stable-8cards-worker-0:00125] MCW rank 1 bound to socket 0[core 6[hwt 0-1]], socket 0[core 7[hwt 0-1]], socket 0[core 8[hwt 0-1]], socket 0[core 9[hwt 0-1]], socket 0[core 10[hwt 0-1]], socket 0[core 11[hwt 0-1]]: [../../../../../../BB/BB/BB/BB/BB/BB/../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../..]
[1,2]<stderr>:[javierhn-stable-8cards-worker-0:00125] MCW rank 2 bound to socket 0[core 12[hwt 0-1]], socket 0[core 13[hwt 0-1]], socket 0[core 14[hwt 0-1]], socket 0[core 15[hwt 0-1]], socket 0[core 16[hwt 0-1]], socket 0[core 17[hwt 0-1]]: [../../../../../../../../../../../../BB/BB/BB/BB/BB/BB/../../../../../..][../../../../../../../../../../../../../../../../../../../../../../../..]
[1,3]<stderr>:[javierhn-stable-8cards-worker-0:00125] MCW rank 3 bound to socket 0[core 18[hwt 0-1]], socket 0[core 19[hwt 0-1]], socket 0[core 20[hwt 0-1]], socket 0[core 21[hwt 0-1]], socket 0[core 22[hwt 0-1]], socket 0[core 23[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../BB/BB/BB/BB/BB/BB][../../../../../../../../../../../../../../../../../../../../../../../..]
[1,4]<stderr>:[javierhn-stable-8cards-worker-0:00125] MCW rank 4 bound to socket 1[core 24[hwt 0-1]], socket 1[core 25[hwt 0-1]], socket 1[core 26[hwt 0-1]], socket 1[core 27[hwt 0-1]], socket 1[core 28[hwt 0-1]], socket 1[core 29[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../..][BB/BB/BB/BB/BB/BB/../../../../../../../../../../../../../../../../../..]
[1,5]<stderr>:[javierhn-stable-8cards-worker-0:00125] MCW rank 5 bound to socket 1[core 30[hwt 0-1]], socket 1[core 31[hwt 0-1]], socket 1[core 32[hwt 0-1]], socket 1[core 33[hwt 0-1]], socket 1[core 34[hwt 0-1]], socket 1[core 35[hwt 0-1]]: [../../../../../../../../../../../../../../../../../../../../../../../..][../../../../../../BB/BB/BB/BB/BB/BB/../../../../../../../../../../../..]
[1,6]<stderr>:[rank: 6] Global seed set to 0
[1,4]<stderr>:[rank: 4] Global seed set to 0
[1,5]<stderr>:[rank: 5] Global seed set to 0
[1,0]<stderr>:[rank: 0] Global seed set to 0
[1,1]<stderr>:[rank: 1] Global seed set to 0
[1,7]<stderr>:[rank: 7] Global seed set to 0
[1,2]<stderr>:[rank: 2] Global seed set to 0
[1,3]<stderr>:[rank: 3] Global seed set to 0
[1,6]<stdout>:LatentDiffusion: Running in eps-prediction mode
[1,7]<stdout>:LatentDiffusion: Running in eps-prediction mode
[1,4]<stdout>:LatentDiffusion: Running in eps-prediction mode
[1,3]<stdout>:LatentDiffusion: Running in eps-prediction mode
[1,5]<stdout>:LatentDiffusion: Running in eps-prediction mode
[1,1]<stdout>:LatentDiffusion: Running in eps-prediction mode
[1,2]<stdout>:LatentDiffusion: Running in eps-prediction mode
[1,0]<stdout>:LatentDiffusion: Running in eps-prediction mode
[1,6]<stdout>:DiffusionWrapper has 859.52 M params.
[1,2]<stdout>:DiffusionWrapper has 859.52 M params.
[1,6]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,6]<stdout>:Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[1,2]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,6]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,2]<stdout>:Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[1,2]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,7]<stdout>:DiffusionWrapper has 859.52 M params.
[1,4]<stdout>:DiffusionWrapper has 859.52 M params.
[1,1]<stdout>:DiffusionWrapper has 859.52 M params.
[1,0]<stdout>:DiffusionWrapper has 859.52 M params.
[1,7]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,4]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,7]<stdout>:Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[1,4]<stdout>:Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[1,7]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,1]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,0]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,4]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,0]<stdout>:Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[1,1]<stdout>:Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[1,0]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,3]<stdout>:DiffusionWrapper has 859.52 M params.
[1,1]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,3]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,3]<stdout>:Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[1,3]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,5]<stdout>:DiffusionWrapper has 859.52 M params.
[1,5]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,5]<stdout>:Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
[1,5]<stdout>:making attention of type 'vanilla' with 512 in_channels
[1,2]<stdout>:Restored from /ceph/datasets/stable_diff/check_point/model.ckpt
[1,6]<stdout>:Restored from /ceph/datasets/stable_diff/check_point/model.ckpt
[1,0]<stdout>:Restored from /ceph/datasets/stable_diff/check_point/model.ckpt
[1,1]<stdout>:Restored from /ceph/datasets/stable_diff/check_point/model.ckpt
[1,7]<stdout>:Restored from /ceph/datasets/stable_diff/check_point/model.ckpt
[1,4]<stdout>:Restored from /ceph/datasets/stable_diff/check_point/model.ckpt
[1,3]<stdout>:Restored from /ceph/datasets/stable_diff/check_point/model.ckpt
[1,5]<stdout>:Restored from /ceph/datasets/stable_diff/check_point/model.ckpt
[1,0]<stderr>:============================= HABANA PT BRIDGE CONFIGURATION =========================== 
[1,0]<stderr>: PT_HPU_LAZY_MODE = 1
[1,0]<stderr>: PT_RECIPE_CACHE_PATH = 
[1,0]<stderr>: PT_CACHE_FOLDER_DELETE = 0
[1,0]<stderr>: PT_HPU_RECIPE_CACHE_CONFIG = 
[1,0]<stderr>: PT_HPU_MAX_COMPOUND_OP_SIZE = 9223372036854775807
[1,0]<stderr>: PT_HPU_LAZY_ACC_PAR_MODE = 1
[1,0]<stderr>: PT_HPU_ENABLE_REFINE_DYNAMIC_SHAPES = 1
[1,0]<stderr>:---------------------------: System Configuration :---------------------------
[1,0]<stderr>:Num CPU Cores : [1,0]<stderr>:96
[1,0]<stderr>:CPU RAM       : 527939964 KB
[1,0]<stderr>:------------------------------------------------------------------------------
[1,6]<stderr>:(…)it-large-patch14/resolve/main/vocab.json:   0%|          | 0.00/961k [00:00<?, ?B/s][1,6]<stderr>:(…)it-large-patch14/resolve/main/vocab.json: 100%|██████████| 961k/961k [00:00<00:00, 4.95MB/s][1,6]<stderr>:(…)it-large-patch14/resolve/main/vocab.json: 100%|██████████| 961k/961k [00:00<00:00, 4.92MB/s]
[1,6]<stderr>:(…)it-large-patch14/resolve/main/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s][1,6]<stderr>:(…)it-large-patch14/resolve/main/merges.txt: 100%|██████████| 525k/525k [00:00<00:00, 2.68MB/s][1,6]<stderr>:(…)it-large-patch14/resolve/main/merges.txt: 100%|██████████| 525k/525k [00:00<00:00, 2.66MB/s]
[1,1]<stderr>:(…)h14/resolve/main/special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s][1,1]<stderr>:(…)h14/resolve/main/special_tokens_map.json: 100%|██████████| 389/389 [00:00<00:00, 1.28MB/s]
[1,3]<stderr>:(…)tch14/resolve/main/tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s][1,3]<stderr>:(…)tch14/resolve/main/tokenizer_config.json: 100%|██████████| 905/905 [00:00<00:00, 3.03MB/s]
[1,6]<stderr>:(…)t-large-patch14/resolve/main/config.json:   0%|          | 0.00/4.52k [00:00<?, ?B/s][1,6]<stderr>:(…)t-large-patch14/resolve/main/config.json: 100%|██████████| 4.52k/4.52k [00:00<00:00, 16.3MB/s]
[1,6]<stderr>:pytorch_model.bin:   0%|          | 0.00/1.71G [00:00<?, ?B/s][1,6]<stderr>:pytorch_model.bin:   2%|▏         | 41.9M/1.71G [00:00<00:04, 361MB/s][1,6]<stderr>:pytorch_model.bin:   6%|▌         | 94.4M/1.71G [00:00<00:03, 420MB/s][1,6]<stderr>:pytorch_model.bin:   9%|▊         | 147M/1.71G [00:00<00:03, 452MB/s] [1,6]<stderr>:pytorch_model.bin:  12%|█▏        | 199M/1.71G [00:00<00:03, 469MB/s][1,6]<stderr>:pytorch_model.bin:  15%|█▍        | 252M/1.71G [00:00<00:03, 475MB/s][1,6]<stderr>:pytorch_model.bin:  18%|█▊        | 304M/1.71G [00:00<00:02, 480MB/s][1,6]<stderr>:pytorch_model.bin:  21%|██        | 357M/1.71G [00:00<00:02, 483MB/s][1,6]<stderr>:pytorch_model.bin:  24%|██▍       | 409M/1.71G [00:00<00:03, 421MB/s][1,6]<stderr>:pytorch_model.bin:  27%|██▋       | 461M/1.71G [00:01<00:03, 411MB/s][1,6]<stderr>:pytorch_model.bin:  30%|███       | 514M/1.71G [00:01<00:03, 394MB/s][1,6]<stderr>:pytorch_model.bin:  33%|███▎      | 566M/1.71G [00:01<00:02, 410MB/s][1,6]<stderr>:pytorch_model.bin:  36%|███▌      | 619M/1.71G [00:01<00:02, 430MB/s][1,6]<stderr>:pytorch_model.bin:  39%|███▉      | 671M/1.71G [00:01<00:02, 431MB/s][1,6]<stderr>:pytorch_model.bin:  42%|████▏     | 724M/1.71G [00:01<00:02, 443MB/s][1,6]<stderr>:pytorch_model.bin:  45%|████▌     | 776M/1.71G [00:01<00:02, 448MB/s][1,6]<stderr>:pytorch_model.bin:  48%|████▊     | 828M/1.71G [00:01<00:02, 426MB/s][1,6]<stderr>:pytorch_model.bin:  51%|█████▏    | 881M/1.71G [00:02<00:02, 342MB/s][1,6]<stderr>:pytorch_model.bin:  54%|█████▍    | 923M/1.71G [00:02<00:02, 354MB/s][1,6]<stderr>:pytorch_model.bin:  57%|█████▋    | 975M/1.71G [00:02<00:01, 385MB/s][1,6]<stderr>:pytorch_model.bin:  59%|█████▉    | 1.02G/1.71G [00:02<00:01, 386MB/s][1,6]<stderr>:pytorch_model.bin:  63%|██████▎   | 1.07G/1.71G [00:02<00:01, 411MB/s][1,6]<stderr>:pytorch_model.bin:  66%|██████▌   | 1.12G/1.71G [00:02<00:01, 429MB/s][1,6]<stderr>:pytorch_model.bin:  69%|██████▊   | 1.17G/1.71G [00:02<00:01, 415MB/s][1,6]<stderr>:pytorch_model.bin:  72%|███████▏  | 1.23G/1.71G [00:02<00:01, 428MB/s][1,6]<stderr>:pytorch_model.bin:  75%|███████▍  | 1.28G/1.71G [00:03<00:01, 403MB/s][1,6]<stderr>:pytorch_model.bin:  78%|███████▊  | 1.33G/1.71G [00:03<00:00, 424MB/s][1,6]<stderr>:pytorch_model.bin:  81%|████████  | 1.38G/1.71G [00:03<00:00, 437MB/s][1,6]<stderr>:pytorch_model.bin:  84%|████████▍ | 1.44G/1.71G [00:03<00:00, 452MB/s][1,6]<stderr>:pytorch_model.bin:  87%|████████▋ | 1.49G/1.71G [00:03<00:00, 462MB/s][1,6]<stderr>:pytorch_model.bin:  90%|█████████ | 1.54G/1.71G [00:03<00:00, 470MB/s][1,6]<stderr>:pytorch_model.bin:  93%|█████████▎| 1.59G/1.71G [00:03<00:00, 478MB/s][1,6]<stderr>:pytorch_model.bin:  96%|█████████▌| 1.65G/1.71G [00:03<00:00, 484MB/s][1,6]<stderr>:pytorch_model.bin:  99%|█████████▉| 1.70G/1.71G [00:03<00:00, 481MB/s][1,6]<stderr>:pytorch_model.bin: 100%|██████████| 1.71G/1.71G [00:03<00:00, 429MB/s]
[1,6]<stderr>:Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_m[1,6]<stderr>:odel.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.enc[1,6]<stderr>:oder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'logit_scale', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.l[1,6]<stderr>:ayers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_[1,6]<stderr>:model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.enco[1,6]<stderr>:der.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias']
[1,6]<stderr>:- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[1,6]<stderr>:- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[1,6]<stdout>:No logger enabled!!!
[1,6]<stdout>:Monitoring val/loss_simple_ema as checkpoint metric.
[1,6]<stdout>:Merged modelckpt-cfg: 
[1,6]<stdout>:{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2023-11-13T23-21-44_hpu_config_web_dataset/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
[1,3]<stderr>:Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight'[1,3]<stderr>:, 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bia[1,3]<stderr>:s', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.[1,3]<stderr>:layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', '[1,3]<stderr>:vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'logit_scale', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_at[1,3]<stderr>:tn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight']
[1,3]<stderr>:- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[1,3]<stderr>:- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[1,7]<stderr>:Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'logit_scale', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.weig[1,7]<stderr>:ht', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'visi[1,7]<stderr>:on_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight',[1,7]<stderr>: 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.[1,7]<stderr>:fc1.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vi[1,7]<stderr>:sion_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight']
[1,7]<stderr>:- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[1,7]<stderr>:- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[1,5]<stderr>:Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.w[1,5]<stderr>:eight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'logit_scale', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.ou[1,5]<stderr>:t_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'visual_projection.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.0.self_[1,5]<stderr>:attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layer[1,5]<stderr>:s.21.layer_norm1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', '[1,5]<stderr>:vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight']
[1,5]<stderr>:- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[1,5]<stderr>:- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[1,4]<stderr>:Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'text_projection.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'visi[1,4]<stderr>:on_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_mo[1,4]<stderr>:del.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.embeddings.position_embedding.weight', 'logit_scale', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_[1,4]<stderr>:model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers[1,4]<stderr>:.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encode[1,4]<stderr>:r.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight']
[1,4]<stderr>:- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[1,4]<stderr>:- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[1,3]<stdout>:No logger enabled!!!
[1,3]<stdout>:Monitoring val/loss_simple_ema as checkpoint metric.
[1,3]<stdout>:Merged modelckpt-cfg: 
[1,3]<stdout>:{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2023-11-13T23-21-44_hpu_config_web_dataset/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
[1,1]<stderr>:Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc[1,1]<stderr>:1.bias', 'visual_projection.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj[1,1]<stderr>:.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.[1,1]<stderr>:encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.[1,1]<stderr>:bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'logit_scale', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.[1,1]<stderr>:encoder.layers.15.self_attn.q_proj.weight', 'text_projection.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias']
[1,1]<stderr>:- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[1,1]<stderr>:- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[1,5]<stdout>:No logger enabled!!!
[1,2]<stderr>:Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'logit_scale', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_mod[1,2]<stderr>:el.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.lay[1,2]<stderr>:ers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2[1,2]<stderr>:.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.lay[1,2]<stderr>:ers.9.layer_norm1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc[1,2]<stderr>:1.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'text_projection.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight']
[1,2]<stderr>:- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[1,2]<stderr>:- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[1,5]<stdout>:Monitoring val/loss_simple_ema as checkpoint metric.
[1,0]<stderr>:Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'text_projection.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encod[1,0]<stderr>:er.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj[1,0]<stderr>:.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_no[1,0]<stderr>:rm1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'logit_scale', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.6.sel[1,0]<stderr>:f_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_at[1,0]<stderr>:tn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight']
[1,0]<stderr>:- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
[1,0]<stderr>:- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[1,5]<stdout>:Merged modelckpt-cfg: 
[1,5]<stdout>:{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2023-11-13T23-21-44_hpu_config_web_dataset/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
[1,4]<stdout>:No logger enabled!!!
[1,7]<stdout>:No logger enabled!!!
[1,4]<stdout>:Monitoring val/loss_simple_ema as checkpoint metric.
[1,7]<stdout>:Monitoring val/loss_simple_ema as checkpoint metric.
[1,4]<stdout>:Merged modelckpt-cfg: 
[1,4]<stdout>:{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2023-11-13T23-21-44_hpu_config_web_dataset/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
[1,7]<stdout>:Merged modelckpt-cfg: 
[1,7]<stdout>:{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2023-11-13T23-21-44_hpu_config_web_dataset/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
[1,1]<stdout>:No logger enabled!!!
[1,1]<stdout>:Monitoring val/loss_simple_ema as checkpoint metric.
[1,1]<stdout>:Merged modelckpt-cfg: 
[1,1]<stdout>:{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2023-11-13T23-21-44_hpu_config_web_dataset/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
[1,0]<stdout>:No logger enabled!!!
[1,0]<stdout>:Monitoring val/loss_simple_ema as checkpoint metric.
[1,0]<stdout>:Merged modelckpt-cfg: 
[1,0]<stdout>:{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2023-11-13T23-21-44_hpu_config_web_dataset/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
[1,2]<stdout>:No logger enabled!!!
[1,2]<stdout>:Monitoring val/loss_simple_ema as checkpoint metric.
[1,2]<stdout>:Merged modelckpt-cfg: 
[1,2]<stdout>:{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2023-11-13T23-21-44_hpu_config_web_dataset/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
[1,0]<stderr>:GPU available: False, used: False
[1,0]<stderr>:TPU available: False, using: 0 TPU cores
[1,0]<stderr>:IPU available: False, using: 0 IPUs
[1,0]<stderr>:HPU available: True, using: 8 HPUs
[1,0]<stderr>:Downloading: "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth" to /scratch/tmp/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth
[1,5]<stderr>:Downloading: "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth" to /scratch/tmp/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth
[1,3]<stderr>:Downloading: "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth" to /scratch/tmp/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth
[1,2]<stderr>:Downloading: "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth" to /scratch/tmp/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth
[1,4]<stderr>:Downloading: "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth" to /scratch/tmp/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth
[1,1]<stderr>:Downloading: "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth" to /scratch/tmp/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth
[1,6]<stderr>:Downloading: "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth" to /scratch/tmp/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth
[1,7]<stderr>:Downloading: "https://github.com/DagnyT/hardnet/raw/master/pretrained/train_liberty_with_aug/checkpoint_liberty_with_aug.pth" to /scratch/tmp/.cache/torch/hub/checkpoints/checkpoint_liberty_with_aug.pth
[1,4]<stderr>:  0%|          | 0.00/5.10M [00:00<?, ?B/s][1,6]<stderr>:  0%|          | 0.00/5.10M [00:00<?, ?B/s][1,4]<stderr>:100%|██████████| 5.10M/5.10M [00:00<00:00, 123MB/s]
[1,6]<stderr>:100%|██████████| 5.10M/5.10M [00:00<00:00, 147MB/s]
[1,3]<stderr>:  0%|          | 0.00/5.10M [00:00<?, ?B/s][1,1]<stderr>:  0%|          | 0.00/5.10M [00:00<?, ?B/s][1,7]<stderr>:  0%|          | 0.00/5.10M [00:00<?, ?B/s][1,5]<stderr>:  0%|          | 0.00/5.10M [00:00<?, ?B/s][1,2]<stderr>:  0%|          | 0.00/5.10M [00:00<?, ?B/s][1,0]<stderr>:  0%|          | 0.00/5.10M [00:00<?, ?B/s][1,3]<stderr>:100%|██████████| 5.10M/5.10M [00:00<00:00, 155MB/s]
[1,1]<stderr>:100%|██████████| 5.10M/5.10M [00:00<00:00, 152MB/s]
[1,7]<stderr>:100%|██████████| 5.10M/5.10M [00:00<00:00, 146MB/s]
[1,5]<stderr>:100%|██████████| 5.10M/5.10M [00:00<00:00, 141MB/s]
[1,2]<stderr>:100%|██████████| 5.10M/5.10M [00:00<00:00, 151MB/s]
[1,0]<stderr>:100%|██████████| 5.10M/5.10M [00:00<00:00, 158MB/s]
[1,3]<stderr>:/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/functional.py:9: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.
[1,3]<stderr>:  from scipy.ndimage.filters import gaussian_filter
[1,2]<stderr>:/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/functional.py:9: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.
[1,2]<stderr>:  from scipy.ndimage.filters import gaussian_filter
[1,4]<stderr>:/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/functional.py:9: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.
[1,4]<stderr>:  from scipy.ndimage.filters import gaussian_filter
[1,1]<stderr>:/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/functional.py:9: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.
[1,1]<stderr>:  from scipy.ndimage.filters import gaussian_filter
[1,7]<stderr>:/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/functional.py:9: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.
[1,7]<stderr>:  from scipy.ndimage.filters import gaussian_filter
[1,0]<stderr>:/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/functional.py:9: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.
[1,0]<stderr>:  from scipy.ndimage.filters import gaussian_filter
[1,5]<stderr>:/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/functional.py:9: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.
[1,5]<stderr>:  from scipy.ndimage.filters import gaussian_filter
[1,6]<stderr>:/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/functional.py:9: DeprecationWarning: Please use `gaussian_filter` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.
[1,6]<stderr>:  from scipy.ndimage.filters import gaussian_filter
[1,2]<stdout>:Setting tar base to /ceph/datasets/stable_diff/laion2B-en
[1,2]<stdout>:#### Data #####
[1,2]<stdout>:datasets not yet initialized.
[1,2]<stdout>:accumulate_grad_batches = 16
[1,2]<stdout>:++++ NOT USING LR SCALING ++++
[1,2]<stdout>:Setting learning rate to 1.00e-04
[1,2]<stderr>:[rank: 2] Global seed set to 0
[1,3]<stdout>:Setting tar base to /ceph/datasets/stable_diff/laion2B-en
[1,3]<stdout>:#### Data #####
[1,3]<stdout>:datasets not yet initialized.
[1,3]<stdout>:accumulate_grad_batches = 16
[1,3]<stdout>:++++ NOT USING LR SCALING ++++
[1,3]<stdout>:Setting learning rate to 1.00e-04
[1,3]<stderr>:[rank: 3] Global seed set to 0
[1,4]<stdout>:Setting tar base to /ceph/datasets/stable_diff/laion2B-en
[1,4]<stdout>:#### Data #####
[1,4]<stdout>:datasets not yet initialized.
[1,4]<stdout>:accumulate_grad_batches = 16
[1,4]<stdout>:++++ NOT USING LR SCALING ++++
[1,4]<stdout>:Setting learning rate to 1.00e-04
[1,6]<stdout>:Setting tar base to /ceph/datasets/stable_diff/laion2B-en
[1,6]<stdout>:#### Data #####
[1,6]<stdout>:datasets not yet initialized.
[1,6]<stdout>:accumulate_grad_batches = 16
[1,4]<stderr>:[rank: 4] Global seed set to 0
[1,6]<stdout>:++++ NOT USING LR SCALING ++++
[1,6]<stdout>:Setting learning rate to 1.00e-04
[1,5]<stdout>:Setting tar base to /ceph/datasets/stable_diff/laion2B-en
[1,5]<stdout>:#### Data #####
[1,5]<stdout>:datasets not yet initialized.
[1,5]<stdout>:accumulate_grad_batches = 16
[1,5]<stdout>:++++ NOT USING LR SCALING ++++
[1,5]<stdout>:Setting learning rate to 1.00e-04
[1,6]<stderr>:[rank: 6] Global seed set to 0
[1,5]<stderr>:[rank: 5] Global seed set to 0
[1,1]<stdout>:Setting tar base to /ceph/datasets/stable_diff/laion2B-en
[1,1]<stdout>:#### Data #####
[1,1]<stdout>:datasets not yet initialized.
[1,1]<stdout>:accumulate_grad_batches = 16
[1,1]<stdout>:++++ NOT USING LR SCALING ++++
[1,1]<stdout>:Setting learning rate to 1.00e-04
[1,1]<stderr>:[rank: 1] Global seed set to 0
[1,7]<stdout>:Setting tar base to /ceph/datasets/stable_diff/laion2B-en
[1,7]<stdout>:#### Data #####
[1,7]<stdout>:datasets not yet initialized.
[1,7]<stdout>:accumulate_grad_batches = 16
[1,7]<stdout>:++++ NOT USING LR SCALING ++++
[1,7]<stdout>:Setting learning rate to 1.00e-04
[1,7]<stderr>:[rank: 7] Global seed set to 0
[1,0]<stdout>:Setting tar base to /ceph/datasets/stable_diff/laion2B-en
[1,0]<stdout>:#### Data #####
[1,0]<stdout>:datasets not yet initialized.
[1,0]<stdout>:accumulate_grad_batches = 16
[1,0]<stdout>:++++ NOT USING LR SCALING ++++
[1,0]<stdout>:Setting learning rate to 1.00e-04
[1,0]<stderr>:[rank: 0] Global seed set to 0
[1,3]<stderr>:Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8
[1,6]<stderr>:Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8
[1,5]<stderr>:Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8
[1,4]<stderr>:Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8
[1,2]<stderr>:Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8
[1,1]<stderr>:Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8
[1,7]<stderr>:Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8
[1,0]<stderr>:Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8
[1,0]<stderr>:----------------------------------------------------------------------------------------------------
[1,0]<stderr>:distributed_backend=hccl
[1,0]<stderr>:All distributed processes registered. Starting with 8 processes
[1,0]<stderr>:----------------------------------------------------------------------------------------------------
[1,0]<stderr>:
[1,5]<stderr>:Missing logger folder: /home/models/Model-References/PyTorch/generative_models/stable-diffusion/lightning_logs
[1,0]<stderr>:Missing logger folder: /home/models/Model-References/PyTorch/generative_models/stable-diffusion/lightning_logs
[1,3]<stderr>:Missing logger folder: /home/models/Model-References/PyTorch/generative_models/stable-diffusion/lightning_logs
[1,2]<stderr>:Missing logger folder: /home/models/Model-References/PyTorch/generative_models/stable-diffusion/lightning_logs
[1,6]<stderr>:Missing logger folder: /home/models/Model-References/PyTorch/generative_models/stable-diffusion/lightning_logs
[1,4]<stderr>:Missing logger folder: /home/models/Model-References/PyTorch/generative_models/stable-diffusion/lightning_logs
[1,1]<stderr>:Missing logger folder: /home/models/Model-References/PyTorch/generative_models/stable-diffusion/lightning_logs
[1,7]<stderr>:Missing logger folder: /home/models/Model-References/PyTorch/generative_models/stable-diffusion/lightning_logs
[1,0]<stderr>:/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
[1,0]<stderr>:  if not hasattr(tensorboard, "__version__") or LooseVersion(
[1,0]<stderr>:/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
[1,0]<stderr>:  ) < LooseVersion("1.15"):
[1,5]<stdout>:Setting up LambdaLR scheduler...
[1,3]<stdout>:Setting up LambdaLR scheduler...
[1,4]<stdout>:Setting up LambdaLR scheduler...
[1,2]<stdout>:Setting up LambdaLR scheduler...
[1,1]<stdout>:Setting up LambdaLR scheduler...
[1,7]<stdout>:Setting up LambdaLR scheduler...
[1,6]<stdout>:Setting up LambdaLR scheduler...
[1,0]<stdout>:Setting up LambdaLR scheduler...
[1,0]<stdout>:Project config
[1,0]<stdout>:model:
[1,0]<stdout>:  base_learning_rate: 0.0001
[1,0]<stdout>:  target: ldm.models.diffusion.ddpm.LatentDiffusion
[1,0]<stdout>:  params:
[1,0]<stdout>:    linear_start: 0.00085
[1,0]<stdout>:    linear_end: 0.012
[1,0]<stdout>:    num_timesteps_cond: 1
[1,0]<stdout>:    log_every_t: 200
[1,0]<stdout>:    timesteps: 1000
[1,0]<stdout>:    first_stage_key: jpg
[1,0]<stdout>:    cond_stage_key: txt
[1,0]<stdout>:    image_size: 32
[1,0]<stdout>:    channels: 4
[1,0]<stdout>:    cond_stage_trainable: false
[1,0]<stdout>:    conditioning_key: crossattn
[1,0]<stdout>:    monitor: val/loss_simple_ema
[1,0]<stdout>:    scale_factor: 0.18215
[1,0]<stdout>:    use_fused_adamw: true
[1,0]<stdout>:    use_ema: false
[1,0]<stdout>:    use_autocast: true
[1,0]<stdout>:    scheduler_config:
[1,0]<stdout>:      target: ldm.lr_scheduler.LambdaLinearScheduler
[1,0]<stdout>:      params:
[1,0]<stdout>:        warm_up_steps:
[1,0]<stdout>:        - 10000
[1,0]<stdout>:        cycle_lengths:
[1,0]<stdout>:        - 10000000000000
[1,0]<stdout>:        f_start:
[1,0]<stdout>:        - 1.0e-06
[1,0]<stdout>:        f_max:
[1,0]<stdout>:        - 1.0
[1,0]<stdout>:        f_min:
[1,0]<stdout>:        - 1.0
[1,0]<stdout>:    unet_config:
[1,0]<stdout>:      target: ldm.modules.diffusionmodules.openaimodel.UNetModel
[1,0]<stdout>:      params:
[1,0]<stdout>:        image_size: 32
[1,0]<stdout>:        in_channels: 4
[1,0]<stdout>:        out_channels: 4
[1,0]<stdout>:        model_channels: 320
[1,0]<stdout>:        attention_resolutions:
[1,0]<stdout>:        - 4
[1,0]<stdout>:        - 2
[1,0]<stdout>:        - 1
[1,0]<stdout>:        num_res_blocks: 2
[1,0]<stdout>:        channel_mult:
[1,0]<stdout>:        - 1
[1,0]<stdout>:        - 2
[1,0]<stdout>:        - 4
[1,0]<stdout>:        - 4
[1,0]<stdout>:        num_heads: 8
[1,0]<stdout>:        use_spatial_transformer: true
[1,0]<stdout>:        transformer_depth: 1
[1,0]<stdout>:        context_dim: 768
[1,0]<stdout>:        use_checkpoint: false
[1,0]<stdout>:        legacy: false
[1,0]<stdout>:    first_stage_config:
[1,0]<stdout>:      target: ldm.models.autoencoder.AutoencoderKL
[1,0]<stdout>:      params:
[1,0]<stdout>:        embed_dim: 4
[1,0]<stdout>:        monitor: val/rec_loss
[1,0]<stdout>:        ckpt_path: /ceph/datasets/stable_diff/check_point/model.ckpt
[1,0]<stdout>:        ddconfig:
[1,0]<stdout>:          double_z: true
[1,0]<stdout>:          z_channels: 4
[1,0]<stdout>:          resolution: 256
[1,0]<stdout>:          in_channels: 3
[1,0]<stdout>:          out_ch: 3
[1,0]<stdout>:          ch: 128
[1,0]<stdout>:          ch_mult:
[1,0]<stdout>:          - 1
[1,0]<stdout>:          - 2
[1,0]<stdout>:          - 4
[1,0]<stdout>:          - 4
[1,0]<stdout>:          num_res_blocks: 2
[1,0]<stdout>:          attn_resolutions: []
[1,0]<stdout>:          dropout: 0.0
[1,0]<stdout>:        lossconfig:
[1,0]<stdout>:          target: torch.nn.Identity
[1,0]<stdout>:    cond_stage_config:
[1,0]<stdout>:      target: ldm.modules.encoders.modules.FrozenCLIPEmbedder
[1,0]<stdout>:      params:
[1,0]<stdout>:        device: hpu
[1,0]<stdout>:    hpu: true
[1,0]<stdout>:    hpu_graph: false
[1,0]<stdout>:data:
[1,0]<stdout>:  target: ldm.data.laion.WebDataModuleFromConfig
[1,0]<stdout>:  params:
[1,0]<stdout>:    tar_base: /software/lfs/data/pytorch/stable-diffusion/laion2B-data
[1,0]<stdout>:    batch_size: 8
[1,0]<stdout>:    num_workers: 4
[1,0]<stdout>:    multinode: true
[1,0]<stdout>:    train:
[1,0]<stdout>:      shards: '{000256..111463}.tar'
[1,0]<stdout>:      shuffle: 10000
[1,0]<stdout>:      image_key: jpg
[1,0]<stdout>:      image_transforms:
[1,0]<stdout>:      - target: torchvision.transforms.Resize
[1,0]<stdout>:        params:
[1,0]<stdout>:          size: 256
[1,0]<stdout>:          interpolation: 3
[1,0]<stdout>:      - target: torchvision.transforms.RandomCrop
[1,0]<stdout>:        params:
[1,0]<stdout>:          size: 256
[1,0]<stdout>:    validation:
[1,0]<stdout>:      shards: '{000000..000255}.tar'
[1,0]<stdout>:      shuffle: 0
[1,0]<stdout>:      image_key: jpg
[1,0]<stdout>:      image_transforms:
[1,0]<stdout>:      - target: torchvision.transforms.Resize
[1,0]<stdout>:        params:
[1,0]<stdout>:          size: 256
[1,0]<stdout>:          interpolation: 3
[1,0]<stdout>:      - target: torchvision.transforms.CenterCrop
[1,0]<stdout>:        params:
[1,0]<stdout>:          size: 256
[1,0]<stdout>:
[1,0]<stdout>:Lightning config
[1,0]<stdout>:trainer:
[1,0]<stdout>:  hmp_bf16: ops_bf16.txt
[1,0]<stdout>:  hmp_fp32: ops_fp32.txt
[1,0]<stdout>:  val_check_interval: 1000
[1,0]<stdout>:  num_sanity_val_steps: 10
[1,0]<stdout>:  benchmark: true
[1,0]<stdout>:  accumulate_grad_batches: 16
[1,0]<stdout>:  max_epochs: 10
[1,0]<stdout>:  max_steps: 237000
[1,0]<stdout>:  limit_val_batches: 0.0
[1,0]<stdout>:  accelerator: hpu
[1,0]<stdout>:  devices: 8
[1,0]<stdout>:  limit_train_batches: 1000.0
[1,0]<stdout>:  num_nodes: 1
[1,0]<stdout>:callbacks:
[1,0]<stdout>:  image_logger:
[1,0]<stdout>:    target: main.ImageLogger
[1,0]<stdout>:    params:
[1,0]<stdout>:      disabled: false
[1,0]<stdout>:      batch_frequency: 50000
[1,0]<stdout>:      max_images: 10
[1,0]<stdout>:      increase_log_steps: false
[1,0]<stdout>:      log_first_step: false
[1,0]<stdout>:      log_images_kwargs:
[1,0]<stdout>:        use_ema_scope: false
[1,0]<stdout>:        inpaint: true
[1,0]<stdout>:        plot_progressive_rows: false
[1,0]<stdout>:        plot_diffusion_rows: true
[1,0]<stdout>:        'N': 4
[1,0]<stdout>:        unconditional_guidance_scale: 3.0
[1,0]<stdout>:        unconditional_guidance_label:
[1,0]<stdout>:        - ''
[1,0]<stdout>:  print_freq:
[1,0]<stdout>:    target: pytorch_lightning.callbacks.TQDMProgressBar
[1,0]<stdout>:    params:
[1,0]<stdout>:      refresh_rate: 100
[1,0]<stdout>:
[1,0]<stderr>:
[1,0]<stderr>:  | Name              | Type               | Params
[1,0]<stderr>:---------------------------------------------------------
[1,0]<stderr>:0 | model             | DiffusionWrapper   | 859 M 
[1,0]<stderr>:1 | first_stage_model | AutoencoderKL      | 83.7 M
[1,0]<stderr>:2 | cond_stage_model  | FrozenCLIPEmbedder | 123 M 
[1,0]<stderr>:---------------------------------------------------------
[1,0]<stderr>:859 M     Trainable params
[1,0]<stderr>:206 M     Non-trainable params
[1,0]<stderr>:1.1 B     Total params
[1,0]<stderr>:4,264.941 Total estimated model params size (MB)
[1,5]<stdout>:Loading webdataset with 111208 shards.
[1,6]<stdout>:Loading webdataset with 111208 shards.
[1,7]<stdout>:Loading webdataset with 111208 shards.
[1,1]<stdout>:Loading webdataset with 111208 shards.
[1,3]<stdout>:Loading webdataset with 111208 shards.
[1,2]<stdout>:Loading webdataset with 111208 shards.
[1,0]<stdout>:Loading webdataset with 111208 shards.
[1,4]<stdout>:Loading webdataset with 111208 shards.
[1,0]<stderr>:/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.
[1,0]<stderr>:  warning_cache.warn(
[1,5]<stderr>:/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:868: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. 
[1,5]<stderr>:  warnings.warn(str(msg))
[1,0]<stderr>:Average Epoch time: 8003.28 seconds
[1,0]<stderr>:Average Peak memory 31106.99 MiB
[1,0]<stderr>:/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:432: PossibleUserWarning: It is recommended to use `self.log('train/loss_simple', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
[1,0]<stderr>:  warning_cache.warn(
[1,0]<stderr>:/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:432: PossibleUserWarning: It is recommended to use `self.log('train/loss_vlb', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
[1,0]<stderr>:  warning_cache.warn(
[1,0]<stderr>:/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:432: PossibleUserWarning: It is recommended to use `self.log('train/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
[1,0]<stderr>:  warning_cache.warn(
[1,0]<stderr>:/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:359: UserWarning: `ModelCheckpoint(monitor='val/loss_simple_ema')` could not find the monitored key in the returned metrics: ['train/loss_simple', 'train/loss_simple_step', 'train/loss_vlb', 'train/loss_vlb_step', 'train/loss', 'train/loss_step', 'global_step', 'lr_abs', 'train/loss_simple_epoch', 'train/loss_vlb_epoch', 'train/loss_epoch', 'epoch', 'step']. HINT: Did you call `log('val/loss_simple_ema', value)` in the `LightningModule`?
[1,0]<stderr>:  warning_cache.warn(m)
[1,0]<stderr>:Epoch 0, global step 63: 'val/loss_simple_ema' was not in top 3
[1,0]<stderr>:Average Epoch time: 4568.62 seconds
[1,0]<stderr>:Average Peak memory 30777.65 MiB
[1,0]<stderr>:Epoch 1, global step 126: 'val/loss_simple_ema' was not in top 3
[1,0]<stderr>:Average Epoch time: 2016.92 seconds
[1,0]<stderr>:Average Peak memory 30969.05 MiB
[1,0]<stdout>:Training: 0it [00:00, ?it/s][1,0]<stdout>:Training:   0% 0/1000 [00:00<?, ?it/s]Epoch 0:   0% 0/1000 [00:00<?, ?it/s] [1,0]<stdout>:Epoch 0:  10% 100/1000 [36:16<5:26:26, 21.76s/it][1,0]<stdout>:Epoch 0:  10% 100/1000 [37:06<5:34:02, 22.27s/it, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00729, train/loss_step=1.000, global_step=0.000, lr_abs=1e-10][1,0]<stdout>:Epoch 0:  20% 200/1000 [48:50<3:15:22, 14.65s/it, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00729, train/loss_step=1.000, global_step=0.000, lr_abs=1e-10][1,0]<stdout>:Epoch 0:  20% 200/1000 [48:50<3:15:22, 14.65s/it, v_num=0, train/loss_simple_step=0.996, train/loss_vlb_step=0.00455, train/loss_step=0.996, global_step=6.000, lr_abs=6.01e-8][1,0]<stdout>:Epoch 0:  30% 300/1000 [1:01:03<2:22:28, 12.21s/it, v_num=0, train/loss_simple_step=0.996, train/loss_vlb_step=0.00455, train/loss_step=0.996, global_step=6.000, lr_abs=6.01e-8][1,0]<stdout>:Epoch 0:  30% 300/1000 [1:01:03<2:22:28, 12.21s/it, v_num=0, train/loss_simple_step=0.996, train/loss_vlb_step=0.00812, train/loss_step=0.996, global_step=12.00, lr_abs=1.2e-7] [1,0]<stdout>:Epoch 0:  40% 400/1000 [1:10:32<1:45:49, 10.58s/it, v_num=0, train/loss_simple_step=0.996, train/loss_vlb_step=0.00812, train/loss_step=0.996, global_step=12.00, lr_abs=1.2e-7][1,0]<stdout>:Epoch 0:  40% 400/1000 [1:10:32<1:45:49, 10.58s/it, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00406, train/loss_step=1.000, global_step=18.00, lr_abs=1.8e-7][1,0]<stdout>:Epoch 0:  50% 500/1000 [1:20:11<1:20:11,  9.62s/it, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.00406, train/loss_step=1.000, global_step=18.00, lr_abs=1.8e-7][1,0]<stdout>:Epoch 0:  50% 500/1000 [1:20:12<1:20:12,  9.62s/it, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.0669, train/loss_step=1.000, global_step=25.00, lr_abs=2.5e-7] [1,0]<stdout>:Epoch 0:  60% 600/1000 [1:30:13<1:00:08,  9.02s/it, v_num=0, train/loss_simple_step=1.000, train/loss_vlb_step=0.0669, train/loss_step=1.000, global_step=25.00, lr_abs=2.5e-7][1,0]<stdout>:Epoch 0:  60% 600/1000 [1:30:13<1:00:08,  9.02s/it, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00595, train/loss_step=1.010, global_step=31.00, lr_abs=3.1e-7][1,0]<stdout>:Epoch 0:  70% 700/1000 [1:42:24<43:53,  8.78s/it, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00595, train/loss_step=1.010, global_step=31.00, lr_abs=3.1e-7]  [1,0]<stdout>:Epoch 0:  70% 700/1000 [1:42:24<43:53,  8.78s/it, v_num=0, train/loss_simple_step=0.992, train/loss_vlb_step=0.00656, train/loss_step=0.992, global_step=37.00, lr_abs=3.7e-7][1,0]<stdout>:Epoch 0:  80% 800/1000 [1:54:59<28:44,  8.62s/it, v_num=0, train/loss_simple_step=0.992, train/loss_vlb_step=0.00656, train/loss_step=0.992, global_step=37.00, lr_abs=3.7e-7][1,0]<stdout>:Epoch 0:  80% 800/1000 [1:55:00<28:45,  8.63s/it, v_num=0, train/loss_simple_step=0.992, train/loss_vlb_step=0.00488, train/loss_step=0.992, global_step=43.00, lr_abs=4.3e-7][1,0]<stdout>:Epoch 0:  90% 900/1000 [2:04:34<13:50,  8.31s/it, v_num=0, train/loss_simple_step=0.992, train/loss_vlb_step=0.00488, train/loss_step=0.992, global_step=43.00, lr_abs=4.3e-7][1,0]<stdout>:Epoch 0:  90% 900/1000 [2:04:34<13:50,  8.31s/it, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00397, train/loss_step=1.010, global_step=50.00, lr_abs=5e-7]  [1,0]<stdout>:Epoch 0: 100% 1000/1000 [2:12:45<00:00,  7.97s/it, v_num=0, train/loss_simple_step=1.010, train/loss_vlb_step=0.00397, train/loss_step=1.010, global_step=50.00, lr_abs=5e-7][1,0]<stdout>:Epoch 0: 100% 1000/1000 [2:12:45<00:00,  7.97s/it, v_num=0, train/loss_simple_step=0.984, train/loss_vlb_step=0.00412, train/loss_step=0.984, global_step=56.00, lr_abs=5.6e-7][1,0]<stdout>:Epoch 0: 100% 1000/1000 [2:13:23<00:00,  8.00s/it, v_num=0, train/loss_simple_step=0.984, train/loss_vlb_step=0.00412, train/loss_step=0.984, global_step=56.00, lr_abs=5.6e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 0:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.984, train/loss_vlb_step=0.00412, train/loss_step=0.984, global_step=56.00, lr_abs=5.6e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998]             [1,0]<stdout>:Epoch 1:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.984, train/loss_vlb_step=0.00412, train/loss_step=0.984, global_step=56.00, lr_abs=5.6e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  10% 100/1000 [06:29<58:26,  3.90s/it, v_num=0, train/loss_simple_step=0.984, train/loss_vlb_step=0.00412, train/loss_step=0.984, global_step=56.00, lr_abs=5.6e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  10% 100/1000 [06:29<58:27,  3.90s/it, v_num=0, train/loss_simple_step=0.984, train/loss_vlb_step=0.00409, train/loss_step=0.984, global_step=63.00, lr_abs=6.3e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  20% 200/1000 [10:41<42:47,  3.21s/it, v_num=0, train/loss_simple_step=0.984, train/loss_vlb_step=0.00409, train/loss_step=0.984, global_step=63.00, lr_abs=6.3e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  20% 200/1000 [10:41<42:47,  3.21s/it, v_num=0, train/loss_simple_step=0.980, train/loss_vlb_step=0.00412, train/loss_step=0.980, global_step=69.00, lr_abs=6.9e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  30% 300/1000 [20:33<47:58,  4.11s/it, v_num=0, train/loss_simple_step=0.980, train/loss_vlb_step=0.00412, train/loss_step=0.980, global_step=69.00, lr_abs=6.9e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  30% 300/1000 [21:23<49:54,  4.28s/it, v_num=0, train/loss_simple_step=0.988, train/loss_vlb_step=0.0356, train/loss_step=0.988, global_step=75.00, lr_abs=7.5e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998] [1,0]<stdout>:Epoch 1:  40% 400/1000 [33:29<50:14,  5.02s/it, v_num=0, train/loss_simple_step=0.988, train/loss_vlb_step=0.0356, train/loss_step=0.988, global_step=75.00, lr_abs=7.5e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  40% 400/1000 [33:29<50:14,  5.02s/it, v_num=0, train/loss_simple_step=0.980, train/loss_vlb_step=0.0112, train/loss_step=0.980, global_step=81.00, lr_abs=8.1e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  50% 500/1000 [44:59<44:59,  5.40s/it, v_num=0, train/loss_simple_step=0.980, train/loss_vlb_step=0.0112, train/loss_step=0.980, global_step=81.00, lr_abs=8.1e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  50% 500/1000 [44:59<44:59,  5.40s/it, v_num=0, train/loss_simple_step=0.977, train/loss_vlb_step=0.00394, train/loss_step=0.977, global_step=88.00, lr_abs=8.8e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  60% 600/1000 [51:16<34:10,  5.13s/it, v_num=0, train/loss_simple_step=0.977, train/loss_vlb_step=0.00394, train/loss_step=0.977, global_step=88.00, lr_abs=8.8e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  60% 600/1000 [51:16<34:10,  5.13s/it, v_num=0, train/loss_simple_step=0.988, train/loss_vlb_step=0.00552, train/loss_step=0.988, global_step=94.00, lr_abs=9.4e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  70% 700/1000 [59:08<25:20,  5.07s/it, v_num=0, train/loss_simple_step=0.988, train/loss_vlb_step=0.00552, train/loss_step=0.988, global_step=94.00, lr_abs=9.4e-7, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  70% 700/1000 [59:08<25:20,  5.07s/it, v_num=0, train/loss_simple_step=0.988, train/loss_vlb_step=0.013, train/loss_step=0.988, global_step=100.0, lr_abs=1e-6, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998]    [1,0]<stdout>:Epoch 1:  80% 800/1000 [1:05:03<16:15,  4.88s/it, v_num=0, train/loss_simple_step=0.988, train/loss_vlb_step=0.013, train/loss_step=0.988, global_step=100.0, lr_abs=1e-6, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  80% 800/1000 [1:05:03<16:15,  4.88s/it, v_num=0, train/loss_simple_step=0.984, train/loss_vlb_step=0.00427, train/loss_step=0.984, global_step=106.0, lr_abs=1.06e-6, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  90% 900/1000 [1:11:11<07:54,  4.75s/it, v_num=0, train/loss_simple_step=0.984, train/loss_vlb_step=0.00427, train/loss_step=0.984, global_step=106.0, lr_abs=1.06e-6, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1:  90% 900/1000 [1:11:11<07:54,  4.75s/it, v_num=0, train/loss_simple_step=0.965, train/loss_vlb_step=0.00409, train/loss_step=0.965, global_step=113.0, lr_abs=1.13e-6, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1: 100% 1000/1000 [1:16:08<00:00,  4.57s/it, v_num=0, train/loss_simple_step=0.965, train/loss_vlb_step=0.00409, train/loss_step=0.965, global_step=113.0, lr_abs=1.13e-6, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998][1,0]<stdout>:Epoch 1: 100% 1000/1000 [1:16:08<00:00,  4.57s/it, v_num=0, train/loss_simple_step=0.980, train/loss_vlb_step=0.0111, train/loss_step=0.980, global_step=119.0, lr_abs=1.19e-6, train/loss_simple_epoch=0.998, train/loss_vlb_epoch=0.0116, train/loss_epoch=0.998] [1,0]<stdout>:Epoch 1: 100% 1000/1000 [1:16:08<00:00,  4.57s/it, v_num=0, train/loss_simple_step=0.980, train/loss_vlb_step=0.0111, train/loss_step=0.980, global_step=119.0, lr_abs=1.19e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 1:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.980, train/loss_vlb_step=0.0111, train/loss_step=0.980, global_step=119.0, lr_abs=1.19e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982]             [1,0]<stdout>:Epoch 2:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.980, train/loss_vlb_step=0.0111, train/loss_step=0.980, global_step=119.0, lr_abs=1.19e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  10% 100/1000 [02:45<24:45,  1.65s/it, v_num=0, train/loss_simple_step=0.980, train/loss_vlb_step=0.0111, train/loss_step=0.980, global_step=119.0, lr_abs=1.19e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  10% 100/1000 [02:45<24:46,  1.65s/it, v_num=0, train/loss_simple_step=0.969, train/loss_vlb_step=0.005, train/loss_step=0.969, global_step=126.0, lr_abs=1.26e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982] [1,0]<stdout>:Epoch 2:  20% 200/1000 [06:46<27:05,  2.03s/it, v_num=0, train/loss_simple_step=0.969, train/loss_vlb_step=0.005, train/loss_step=0.969, global_step=126.0, lr_abs=1.26e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  20% 200/1000 [06:46<27:05,  2.03s/it, v_num=0, train/loss_simple_step=0.980, train/loss_vlb_step=0.00623, train/loss_step=0.980, global_step=132.0, lr_abs=1.32e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  30% 300/1000 [12:18<28:43,  2.46s/it, v_num=0, train/loss_simple_step=0.980, train/loss_vlb_step=0.00623, train/loss_step=0.980, global_step=132.0, lr_abs=1.32e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  30% 300/1000 [12:18<28:43,  2.46s/it, v_num=0, train/loss_simple_step=0.977, train/loss_vlb_step=0.00458, train/loss_step=0.977, global_step=138.0, lr_abs=1.38e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  40% 400/1000 [15:16<22:54,  2.29s/it, v_num=0, train/loss_simple_step=0.977, train/loss_vlb_step=0.00458, train/loss_step=0.977, global_step=138.0, lr_abs=1.38e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  40% 400/1000 [15:16<22:54,  2.29s/it, v_num=0, train/loss_simple_step=0.969, train/loss_vlb_step=0.00409, train/loss_step=0.969, global_step=144.0, lr_abs=1.44e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  50% 500/1000 [18:49<18:49,  2.26s/it, v_num=0, train/loss_simple_step=0.969, train/loss_vlb_step=0.00409, train/loss_step=0.969, global_step=144.0, lr_abs=1.44e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  50% 500/1000 [18:49<18:49,  2.26s/it, v_num=0, train/loss_simple_step=0.957, train/loss_vlb_step=0.00439, train/loss_step=0.957, global_step=151.0, lr_abs=1.51e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  60% 600/1000 [23:16<15:31,  2.33s/it, v_num=0, train/loss_simple_step=0.957, train/loss_vlb_step=0.00439, train/loss_step=0.957, global_step=151.0, lr_abs=1.51e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  60% 600/1000 [23:16<15:31,  2.33s/it, v_num=0, train/loss_simple_step=0.945, train/loss_vlb_step=0.00397, train/loss_step=0.945, global_step=157.0, lr_abs=1.57e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  70% 700/1000 [25:34<10:57,  2.19s/it, v_num=0, train/loss_simple_step=0.945, train/loss_vlb_step=0.00397, train/loss_step=0.945, global_step=157.0, lr_abs=1.57e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  70% 700/1000 [25:34<10:57,  2.19s/it, v_num=0, train/loss_simple_step=0.953, train/loss_vlb_step=0.00383, train/loss_step=0.953, global_step=163.0, lr_abs=1.63e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  80% 800/1000 [28:11<07:02,  2.11s/it, v_num=0, train/loss_simple_step=0.953, train/loss_vlb_step=0.00383, train/loss_step=0.953, global_step=163.0, lr_abs=1.63e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  80% 800/1000 [28:11<07:02,  2.11s/it, v_num=0, train/loss_simple_step=0.949, train/loss_vlb_step=0.0123, train/loss_step=0.949, global_step=169.0, lr_abs=1.69e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982] [1,0]<stdout>:Epoch 2:  90% 900/1000 [31:31<03:30,  2.10s/it, v_num=0, train/loss_simple_step=0.949, train/loss_vlb_step=0.0123, train/loss_step=0.949, global_step=169.0, lr_abs=1.69e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2:  90% 900/1000 [31:31<03:30,  2.10s/it, v_num=0, train/loss_simple_step=0.938, train/loss_vlb_step=0.00349, train/loss_step=0.938, global_step=176.0, lr_abs=1.76e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2: 100% 1000/1000 [33:00<00:00,  1.98s/it, v_num=0, train/loss_simple_step=0.938, train/loss_vlb_step=0.00349, train/loss_step=0.938, global_step=176.0, lr_abs=1.76e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982][1,0]<stdout>:Epoch 2: 100% 1000/1000 [33:00<00:00,  1.98s/it, v_num=0, train/loss_simple_step=0.930, train/loss_vlb_step=0.0061, train/loss_step=0.930, global_step=182.0, lr_abs=1.82e-6, train/loss_simple_epoch=0.982, train/loss_vlb_epoch=0.0097, train/loss_epoch=0.982] [1,0]<stdout>:Epoch 2: 100% 1000/1000 [33:36<00:00,  2.02s/it, v_num=0, train/loss_simpl[1,0]<stderr>:Epoch 2, global step 189: 'val/loss_simple_ema' was not in top 3
[1,0]<stderr>:Average Epoch time: 2494.86 seconds
[1,0]<stderr>:Average Peak memory 30940.45 MiB
[1,0]<stderr>:Epoch 3, global step 252: 'val/loss_simple_ema' was not in top 3
[1,0]<stderr>:Average Epoch time: 2189.28 seconds
[1,0]<stderr>:Average Peak memory 30587.66 MiB
[1,0]<stderr>:Epoch 4, global step 315: 'val/loss_simple_ema' was not in top 3
e_step=0.930, train/loss_vlb_step=0.0061, train/loss_step=0.930, global_step=182.0, lr_abs=1.82e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 2:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.930, train/loss_vlb_step=0.0061, train/loss_step=0.930, global_step=182.0, lr_abs=1.82e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957]           [1,0]<stdout>:Epoch 3:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.930, train/loss_vlb_step=0.0061, train/loss_step=0.930, global_step=182.0, lr_abs=1.82e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  10% 100/1000 [01:52<16:50,  1.12s/it, v_num=0, train/loss_simple_step=0.930, train/loss_vlb_step=0.0061, train/loss_step=0.930, global_step=182.0, lr_abs=1.82e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  10% 100/1000 [01:52<16:51,  1.12s/it, v_num=0, train/loss_simple_step=0.926, train/loss_vlb_step=0.00427, train/loss_step=0.926, global_step=189.0, lr_abs=1.89e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  20% 200/1000 [04:14<16:58,  1.27s/it, v_num=0, train/loss_simple_step=0.926, train/loss_vlb_step=0.00427, train/loss_step=0.926, global_step=189.0, lr_abs=1.89e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  20% 200/1000 [04:14<16:58,  1.27s/it, v_num=0, train/loss_simple_step=0.922, train/loss_vlb_step=0.00377, train/loss_step=0.922, global_step=195.0, lr_abs=1.95e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  30% 300/1000 [06:19<14:46,  1.27s/it, v_num=0, train/loss_simple_step=0.922, train/loss_vlb_step=0.00377, train/loss_step=0.922, global_step=195.0, lr_abs=1.95e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  30% 300/1000 [06:19<14:46,  1.27s/it, v_num=0, train/loss_simple_step=0.918, train/loss_vlb_step=0.00397, train/loss_step=0.918, global_step=201.0, lr_abs=2.01e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  40% 400/1000 [10:06<15:09,  1.52s/it, v_num=0, train/loss_simple_step=0.918, train/loss_vlb_step=0.00397, train/loss_step=0.918, global_step=201.0, lr_abs=2.01e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  40% 400/1000 [10:06<15:09,  1.52s/it, v_num=0, train/loss_simple_step=0.910, train/loss_vlb_step=0.00452, train/loss_step=0.910, global_step=207.0, lr_abs=2.07e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  50% 500/1000 [21:53<21:53,  2.63s/it, v_num=0, train/loss_simple_step=0.910, train/loss_vlb_step=0.00452, train/loss_step=0.910, global_step=207.0, lr_abs=2.07e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  50% 500/1000 [21:53<21:53,  2.63s/it, v_num=0, train/loss_simple_step=0.898, train/loss_vlb_step=0.00383, train/loss_step=0.898, global_step=214.0, lr_abs=2.14e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  60% 600/1000 [24:35<16:23,  2.46s/it, v_num=0, train/loss_simple_step=0.898, train/loss_vlb_step=0.00383, train/loss_step=0.898, global_step=214.0, lr_abs=2.14e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  60% 600/1000 [24:35<16:23,  2.46s/it, v_num=0, train/loss_simple_step=0.898, train/loss_vlb_step=0.00424, train/loss_step=0.898, global_step=220.0, lr_abs=2.2e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957] [1,0]<stdout>:Epoch 3:  70% 700/1000 [29:08<12:29,  2.50s/it, v_num=0, train/loss_simple_step=0.898, train/loss_vlb_step=0.00424, train/loss_step=0.898, global_step=220.0, lr_abs=2.2e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  70% 700/1000 [29:08<12:29,  2.50s/it, v_num=0, train/loss_simple_step=0.887, train/loss_vlb_step=0.00359, train/loss_step=0.887, global_step=226.0, lr_abs=2.26e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  80% 800/1000 [32:58<08:14,  2.47s/it, v_num=0, train/loss_simple_step=0.887, train/loss_vlb_step=0.00359, train/loss_step=0.887, global_step=226.0, lr_abs=2.26e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  80% 800/1000 [32:58<08:14,  2.47s/it, v_num=0, train/loss_simple_step=0.898, train/loss_vlb_step=0.00427, train/loss_step=0.898, global_step=232.0, lr_abs=2.32e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  90% 900/1000 [39:44<04:24,  2.65s/it, v_num=0, train/loss_simple_step=0.898, train/loss_vlb_step=0.00427, train/loss_step=0.898, global_step=232.0, lr_abs=2.32e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3:  90% 900/1000 [39:44<04:24,  2.65s/it, v_num=0, train/loss_simple_step=0.875, train/loss_vlb_step=0.0036, train/loss_step=0.875, global_step=239.0, lr_abs=2.39e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957] [1,0]<stdout>:Epoch 3: 100% 1000/1000 [41:34<00:00,  2.49s/it, v_num=0, train/loss_simple_step=0.875, train/loss_vlb_step=0.0036, train/loss_step=0.875, global_step=239.0, lr_abs=2.39e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3: 100% 1000/1000 [41:34<00:00,  2.49s/it, v_num=0, train/loss_simple_step=0.863, train/loss_vlb_step=0.00363, train/loss_step=0.863, global_step=245.0, lr_abs=2.45e-6, train/loss_simple_epoch=0.957, train/loss_vlb_epoch=0.0054, train/loss_epoch=0.957][1,0]<stdout>:Epoch 3: 100% 1000/1000 [41:34<00:00,  2.49s/it, v_num=0, train/loss_simple_step=0.863, train/loss_vlb_step=0.00363, train/loss_step=0.863, global_step=245.0, lr_abs=2.45e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 3:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.863, train/loss_vlb_step=0.00363, train/loss_step=0.863, global_step=245.0, lr_abs=2.45e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900]           [1,0]<stdout>:Epoch 4:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.863, train/loss_vlb_step=0.00363, train/loss_step=0.863, global_step=245.0, lr_abs=2.45e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  10% 100/1000 [03:13<28:59,  1.93s/it, v_num=0, train/loss_simple_step=0.863, train/loss_vlb_step=0.00363, train/loss_step=0.863, global_step=245.0, lr_abs=2.45e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  10% 100/1000 [03:13<29:00,  1.93s/it, v_num=0, train/loss_simple_step=0.852, train/loss_vlb_step=0.00577, train/loss_step=0.852, global_step=252.0, lr_abs=2.52e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  20% 200/1000 [05:43<22:54,  1.72s/it, v_num=0, train/loss_simple_step=0.852, train/loss_vlb_step=0.00577, train/loss_step=0.852, global_step=252.0, lr_abs=2.52e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  20% 200/1000 [05:43<22:55,  1.72s/it, v_num=0, train/loss_simple_step=0.879, train/loss_vlb_step=0.0127, train/loss_step=0.879, global_step=258.0, lr_abs=2.58e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900] [1,0]<stdout>:Epoch 4:  30% 300/1000 [08:37<20:07,  1.73s/it, v_num=0, train/loss_simple_step=0.879, train/loss_vlb_step=0.0127, train/loss_step=0.879, global_step=258.0, lr_abs=2.58e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  30% 300/1000 [08:37<20:07,  1.73s/it, v_num=0, train/loss_simple_step=0.844, train/loss_vlb_step=0.00555, train/loss_step=0.844, global_step=264.0, lr_abs=2.64e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  40% 400/1000 [12:55<19:23,  1.94s/it, v_num=0, train/loss_simple_step=0.844, train/loss_vlb_step=0.00555, train/loss_step=0.844, global_step=264.0, lr_abs=2.64e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  40% 400/1000 [12:55<19:23,  1.94s/it, v_num=0, train/loss_simple_step=0.824, train/loss_vlb_step=0.00365, train/loss_step=0.824, global_step=270.0, lr_abs=2.7e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900] [1,0]<stdout>:Epoch 4:  50% 500/1000 [16:13<16:13,  1.95s/it, v_num=0, train/loss_simple_step=0.824, train/loss_vlb_step=0.00365, train/loss_step=0.824, global_step=270.0, lr_abs=2.7e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  50% 500/1000 [16:13<16:13,  1.95s/it, v_num=0, train/loss_simple_step=0.816, train/loss_vlb_step=0.00394, train/loss_step=0.816, global_step=277.0, lr_abs=2.77e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  60% 600/1000 [19:31<13:01,  1.95s/it, v_num=0, train/loss_simple_step=0.816, train/loss_vlb_step=0.00394, train/loss_step=0.816, global_step=277.0, lr_abs=2.77e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  60% 600/1000 [19:32<13:01,  1.95s/it, v_num=0, train/loss_simple_step=0.812, train/loss_vlb_step=0.00397, train/loss_step=0.812, global_step=283.0, lr_abs=2.83e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  70% 700/1000 [25:03<10:44,  2.15s/it, v_num=0, train/loss_simple_step=0.812, train/loss_vlb_step=0.00397, train/loss_step=0.812, global_step=283.0, lr_abs=2.83e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  70% 700/1000 [25:03<10:44,  2.15s/it, v_num=0, train/loss_simple_step=0.840, train/loss_vlb_step=0.00491, train/loss_step=0.840, global_step=289.0, lr_abs=2.89e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  80% 800/1000 [27:46<06:56,  2.08s/it, v_num=0, train/loss_simple_step=0.840, train/loss_vlb_step=0.00491, train/loss_step=0.840, global_step=289.0, lr_abs=2.89e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  80% 800/1000 [27:46<06:56,  2.08s/it, v_num=0, train/loss_simple_step=0.797, train/loss_vlb_step=0.00403, train/loss_step=0.797, global_step=295.0, lr_abs=2.95e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  90% 900/1000 [34:32<03:50,  2.30s/it, v_num=0, train/loss_simple_step=0.797, train/loss_vlb_step=0.00403, train/loss_step=0.797, global_step=295.0, lr_abs=2.95e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4:  90% 900/1000 [34:32<03:50,  2.30s/it, v_num=0, train/loss_simple_step=0.789, train/loss_vlb_step=0.0031, train/loss_step=0.789, global_step=302.0, lr_abs=3.02e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900] [1,0]<stdout>:Epoch 4: 100% 1000/1000 [36:28<00:00,  2.19s/it, v_num=0, train/loss_simple_step=0.789, train/loss_vlb_step=0.0031, train/loss_step=0.789, global_step=302.0, lr_abs=3.02e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4: 100% 1000/1000 [36:28<00:00,  2.19s/it, v_num=0, train/loss_simple_step=0.754, train/loss_vlb_step=0.00333, train/loss_step=0.754, global_step=308.0, lr_abs=3.08e-6, train/loss_simple_epoch=0.900, train/loss_vlb_epoch=0.00397, train/loss_epoch=0.900][1,0]<stdout>:Epoch 4: 100% 1000/1000 [36:29<00:00,  2.19s/it, v_num=0, train/loss_simple_step=0.754, train/loss_vlb_step=0.00333, train/loss_step=0.754, global_step=308.0, lr_abs=3.08e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 4:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.754, train/loss_vlb_step=0.00333, train/loss_step=0.754, global_step=308.0, lr_abs=3.08e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821]           [1,0]<stdout>:Epoch 5:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.754, train/loss_vlb_step=0.00333, train/loss_step=0.754, global_step=308.0, lr_abs=3.08e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  10% 100/1000 [02:23<21:35,  1.44s/it, v_num=0, train/loss_simple_step=0.754, train/loss_vlb_step=0.00333, train/loss_step=0.754, global_step=308.0, lr_abs=3.08e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  10% 100/1000 [02:24<21:36,  1.44s/it, v_num=0, train/loss_simple_step=0.762, train/loss_vlb_step=0.00304, train/loss_step=0.762, global_step=315.0, lr_abs=3.15e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  20% 200/1000 [04:52<19:28,  1.46s/it, v_num=0, train/loss_simple_step=0.762, train/loss_vlb_step=0.00304, train/loss_step=0.762, global_step=315.0, lr_abs=3.15e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  20% 200/1000 [04:52<19:29,  1.46s/it, v_num=0, train/loss_simple_step=0.781, train/loss_vlb_step=0.00702, train/loss_step=0.781, global_step=321.0, lr_abs=3.21e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  30% 300/1000 [06:13<14:30,  1.24s/it, v_num=0, train/loss_simple_step=0.781, train/loss_vlb_step=0.00702, train/loss_step=0.781, global_step=321.0, lr_abs=3.21e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  30% 300/1000 [06:13<14:30,  1.24s/it, v_num=0, train/loss_simple_step=0.754, train/loss_vlb_step=0.00346, train/loss_step=0.754, global_step=327.0, lr_abs=3.27e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  40% 400/1000 [08:57<13:26,  1.34s/it, v_num=0, train/loss_simple_step=0.754, train/loss_vlb_step=0.00346, train/loss_step=0.754, global_step=327.0, lr_abs=3.27e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  40% 400/1000 [08:57<13:26,  1.34s/it, v_num=0, train/loss_simple_step=0.770, train/loss_vlb_step=0.00607, train/loss_step=0.770, global_step=333.0, lr_abs=3.33e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  50% 500/1000 [14:28<14:28,  1.74s/it, v_num=0, train/loss_simple_step=0.770, train/loss_vlb_step=0.00607, train/loss_step=0.770, global_step=333.0, lr_abs=3.33e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  50% 500/1000 [14:28<14:28,  1.74s/it, v_num=0, train/loss_simple_step=0.770, train/loss_vlb_step=0.00717, train/loss_step=0.770, global_step=340.0, lr_abs=3.4e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821] [1,0]<stdout>:Epoch 5:  60% 600/1000 [16:17<10:51,  1.63s/it, v_num=0, train/loss_simple_step=0.770, train/loss_vlb_step=0.00717, train/loss_step=0.770, global_step=340.0, lr_abs=3.4e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  60% 600/1000 [16:17<10:51,  1.63s/it, v_num=0, train/loss_simple_step=0.723, train/loss_vlb_step=0.00461, train/loss_step[1,1]<stderr>:/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:868: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. 
[1,1]<stderr>:  warnings.warn(str(msg))
[1,0]<stderr>:Average Epoch time: 1611.56 seconds
[1,0]<stderr>:Average Peak memory 30905.52 MiB
[1,0]<stderr>:Epoch 5, global step 378: 'val/loss_simple_ema' was not in top 3
[1,5]<stderr>:/usr/local/lib/python3.10/dist-packages/webdataset/handlers.py:33: UserWarning: OSError('image file is truncated (13 bytes not processed)')
[1,5]<stderr>:  warnings.warn(repr(exn))
[1,0]<stderr>:Average Epoch time: 1333.36 seconds
[1,0]<stderr>:Average Peak memory 30457.52 MiB
[1,0]<stderr>:Epoch 6, global step 441: 'val/loss_simple_ema' was not in top 3
[1,2]<stderr>:/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:868: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. 
[1,2]<stderr>:  warnings.warn(str(msg))
[1,0]<stderr>:Average Epoch time: 1270.41 seconds
[1,0]<stderr>:Average Peak memory 30514.57 MiB
[1,0]<stderr>:Epoch 7, global step 504: 'val/loss_simple_ema' was not in top 3
=0.723, global_step=346.0, lr_abs=3.46e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  70% 700/1000 [19:12<08:13,  1.65s/it, v_num=0, train/loss_simple_step=0.723, train/loss_vlb_step=0.00461, train/loss_step=0.723, global_step=346.0, lr_abs=3.46e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  70% 700/1000 [19:12<08:13,  1.65s/it, v_num=0, train/loss_simple_step=0.715, train/loss_vlb_step=0.00314, train/loss_step=0.715, global_step=352.0, lr_abs=3.52e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  80% 800/1000 [22:12<05:33,  1.67s/it, v_num=0, train/loss_simple_step=0.715, train/loss_vlb_step=0.00314, train/loss_step=0.715, global_step=352.0, lr_abs=3.52e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  80% 800/1000 [22:12<05:33,  1.67s/it, v_num=0, train/loss_simple_step=0.691, train/loss_vlb_step=0.00278, train/loss_step=0.691, global_step=358.0, lr_abs=3.58e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  90% 900/1000 [24:39<02:44,  1.64s/it, v_num=0, train/loss_simple_step=0.691, train/loss_vlb_step=0.00278, train/loss_step=0.691, global_step=358.0, lr_abs=3.58e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5:  90% 900/1000 [24:39<02:44,  1.64s/it, v_num=0, train/loss_simple_step=0.695, train/loss_vlb_step=0.00316, train/loss_step=0.695, global_step=365.0, lr_abs=3.65e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5: 100% 1000/1000 [26:51<00:00,  1.61s/it, v_num=0, train/loss_simple_step=0.695, train/loss_vlb_step=0.00316, train/loss_step=0.695, global_step=365.0, lr_abs=3.65e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5: 100% 1000/1000 [26:51<00:00,  1.61s/it, v_num=0, train/loss_simple_step=0.695, train/loss_vlb_step=0.00339, train/loss_step=0.695, global_step=371.0, lr_abs=3.71e-6, train/loss_simple_epoch=0.821, train/loss_vlb_epoch=0.00509, train/loss_epoch=0.821][1,0]<stdout>:Epoch 5: 100% 1000/1000 [26:51<00:00,  1.61s/it, v_num=0, train/loss_simple_step=0.695, train/loss_vlb_step=0.00339, train/loss_step=0.695, global_step=371.0, lr_abs=3.71e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 5:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.695, train/loss_vlb_step=0.00339, train/loss_step=0.695, global_step=371.0, lr_abs=3.71e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736]           [1,0]<stdout>:Epoch 6:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.695, train/loss_vlb_step=0.00339, train/loss_step=0.695, global_step=371.0, lr_abs=3.71e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  10% 100/1000 [01:16<11:25,  1.31it/s, v_num=0, train/loss_simple_step=0.695, train/loss_vlb_step=0.00339, train/loss_step=0.695, global_step=371.0, lr_abs=3.71e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  10% 100/1000 [01:16<11:26,  1.31it/s, v_num=0, train/loss_simple_step=0.680, train/loss_vlb_step=0.00343, train/loss_step=0.680, global_step=378.0, lr_abs=3.78e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  20% 200/1000 [03:07<12:28,  1.07it/s, v_num=0, train/loss_simple_step=0.680, train/loss_vlb_step=0.00343, train/loss_step=0.680, global_step=378.0, lr_abs=3.78e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  20% 200/1000 [03:07<12:28,  1.07it/s, v_num=0, train/loss_simple_step=0.656, train/loss_vlb_step=0.00258, train/loss_step=0.656, global_step=384.0, lr_abs=3.84e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  30% 300/1000 [04:36<10:44,  1.09it/s, v_num=0, train/loss_simple_step=0.656, train/loss_vlb_step=0.00258, train/loss_step=0.656, global_step=384.0, lr_abs=3.84e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  30% 300/1000 [04:36<10:44,  1.09it/s, v_num=0, train/loss_simple_step=0.652, train/loss_vlb_step=0.00299, train/loss_step=0.652, global_step=390.0, lr_abs=3.9e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736] [1,0]<stdout>:Epoch 6:  40% 400/1000 [06:57<10:26,  1.04s/it, v_num=0, train/loss_simple_step=0.652, train/loss_vlb_step=0.00299, train/loss_step=0.652, global_step=390.0, lr_abs=3.9e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  40% 400/1000 [06:57<10:26,  1.04s/it, v_num=0, train/loss_simple_step=0.664, train/loss_vlb_step=0.00281, train/loss_step=0.664, global_step=396.0, lr_abs=3.96e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  50% 500/1000 [09:11<09:11,  1.10s/it, v_num=0, train/loss_simple_step=0.664, train/loss_vlb_step=0.00281, train/loss_step=0.664, global_step=396.0, lr_abs=3.96e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  50% 500/1000 [09:11<09:11,  1.10s/it, v_num=0, train/loss_simple_step=0.742, train/loss_vlb_step=0.0223, train/loss_step=0.742, global_step=403.0, lr_abs=4.03e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736] [1,0]<stdout>:Epoch 6:  60% 600/1000 [12:14<08:09,  1.22s/it, v_num=0, train/loss_simple_step=0.742, train/loss_vlb_step=0.0223, train/loss_step=0.742, global_step=403.0, lr_abs=4.03e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  60% 600/1000 [12:15<08:10,  1.23s/it, v_num=0, train/loss_simple_step=0.613, train/loss_vlb_step=0.00247, train/loss_step=0.613, global_step=409.0, lr_abs=4.09e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  70% 700/1000 [12:52<05:31,  1.10s/it, v_num=0, train/loss_simple_step=0.613, train/loss_vlb_step=0.00247, train/loss_step=0.613, global_step=409.0, lr_abs=4.09e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  70% 700/1000 [12:52<05:31,  1.10s/it, v_num=0, train/loss_simple_step=0.703, train/loss_vlb_step=0.00368, train/loss_step=0.703, global_step=415.0, lr_abs=4.15e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  80% 800/1000 [16:10<04:02,  1.21s/it, v_num=0, train/loss_simple_step=0.703, train/loss_vlb_step=0.00368, train/loss_step=0.703, global_step=415.0, lr_abs=4.15e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  80% 800/1000 [16:10<04:02,  1.21s/it, v_num=0, train/loss_simple_step=0.703, train/loss_vlb_step=0.0181, train/loss_step=0.703, global_step=421.0, lr_abs=4.21e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736] [1,0]<stdout>:Epoch 6:  90% 900/1000 [19:46<02:11,  1.32s/it, v_num=0, train/loss_simple_step=0.703, train/loss_vlb_step=0.0181, train/loss_step=0.703, global_step=421.0, lr_abs=4.21e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6:  90% 900/1000 [19:46<02:11,  1.32s/it, v_num=0, train/loss_simple_step=0.637, train/loss_vlb_step=0.00337, train/loss_step=0.637, global_step=428.0, lr_abs=4.28e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6: 100% 1000/1000 [22:13<00:00,  1.33s/it, v_num=0, train/loss_simple_step=0.637, train/loss_vlb_step=0.00337, train/loss_step=0.637, global_step=428.0, lr_abs=4.28e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6: 100% 1000/1000 [22:13<00:00,  1.33s/it, v_num=0, train/loss_simple_step=0.609, train/loss_vlb_step=0.00339, train/loss_step=0.609, global_step=434.0, lr_abs=4.34e-6, train/loss_simple_epoch=0.736, train/loss_vlb_epoch=0.00438, train/loss_epoch=0.736][1,0]<stdout>:Epoch 6: 100% 1000/1000 [22:13<00:00,  1.33s/it, v_num=0, train/loss_simple_step=0.609, train/loss_vlb_step=0.00339, train/loss_step=0.609, global_step=434.0, lr_abs=4.34e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 6:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.609, train/loss_vlb_step=0.00339, train/loss_step=0.609, global_step=434.0, lr_abs=4.34e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666]           [1,0]<stdout>:Epoch 7:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.609, train/loss_vlb_step=0.00339, train/loss_step=0.609, global_step=434.0, lr_abs=4.34e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  10% 100/1000 [03:04<27:37,  1.84s/it, v_num=0, train/loss_simple_step=0.609, train/loss_vlb_step=0.00339, train/loss_step=0.609, global_step=434.0, lr_abs=4.34e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  10% 100/1000 [03:04<27:38,  1.84s/it, v_num=0, train/loss_simple_step=0.590, train/loss_vlb_step=0.00238, train/loss_step=0.590, global_step=441.0, lr_abs=4.41e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  20% 200/1000 [04:37<18:28,  1.39s/it, v_num=0, train/loss_simple_step=0.590, train/loss_vlb_step=0.00238, train/loss_step=0.590, global_step=441.0, lr_abs=4.41e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  20% 200/1000 [04:37<18:28,  1.39s/it, v_num=0, train/loss_simple_step=0.605, train/loss_vlb_step=0.00409, train/loss_step=0.605, global_step=447.0, lr_abs=4.47e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  30% 300/1000 [06:44<15:43,  1.35s/it, v_num=0, train/loss_simple_step=0.605, train/loss_vlb_step=0.00409, train/loss_step=0.605, global_step=447.0, lr_abs=4.47e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  30% 300/1000 [06:44<15:43,  1.35s/it, v_num=0, train/loss_simple_step=0.582, train/loss_vlb_step=0.00293, train/loss_step=0.582, global_step=453.0, lr_abs=4.53e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  40% 400/1000 [09:23<14:05,  1.41s/it, v_num=0, train/loss_simple_step=0.582, train/loss_vlb_step=0.00293, train/loss_step=0.582, global_step=453.0, lr_abs=4.53e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  40% 400/1000 [09:23<14:05,  1.41s/it, v_num=0, train/loss_simple_step=0.562, train/loss_vlb_step=0.00302, train/loss_step=0.562, global_step=459.0, lr_abs=4.59e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  50% 500/1000 [11:49<11:49,  1.42s/it, v_num=0, train/loss_simple_step=0.562, train/loss_vlb_step=0.00302, train/loss_step=0.562, global_step=459.0, lr_abs=4.59e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  50% 500/1000 [11:49<11:49,  1.42s/it, v_num=0, train/loss_simple_step=0.539, train/loss_vlb_step=0.00209, train/loss_step=0.539, global_step=466.0, lr_abs=4.66e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  60% 600/1000 [13:02<08:41,  1.30s/it, v_num=0, train/loss_simple_step=0.539, train/loss_vlb_step=0.00209, train/loss_step=0.539, global_step=466.0, lr_abs=4.66e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  60% 600/1000 [13:02<08:41,  1.30s/it, v_num=0, train/loss_simple_step=0.547, train/loss_vlb_step=0.00229, train/loss_step=0.547, global_step=472.0, lr_abs=4.72e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  70% 700/1000 [14:31<06:13,  1.24s/it, v_num=0, train/loss_simple_step=0.547, train/loss_vlb_step=0.00229, train/loss_step=0.547, global_step=472.0, lr_abs=4.72e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  70% 700/1000 [14:31<06:13,  1.25s/it, v_num=0, train/loss_simple_step=0.598, train/loss_vlb_step=0.00302, train/loss_step=0.598, global_step=478.0, lr_abs=4.78e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  80% 800/1000 [18:10<04:32,  1.36s/it, v_num=0, train/loss_simple_step=0.598, train/loss_vlb_step=0.00302, train/loss_step=0.598, global_step=478.0, lr_abs=4.78e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  80% 800/1000 [18:10<04:32,  1.36s/it, v_num=0, train/loss_simple_step=0.531, train/loss_vlb_step=0.00322, train/loss_step=0.531, global_step=484.0, lr_abs=4.84e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  90% 900/1000 [19:40<02:11,  1.31s/it, v_num=0, train/loss_simple_step=0.531, train/loss_vlb_step=0.00322, train/loss_step=0.531, global_step=484.0, lr_abs=4.84e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7:  90% 900/1000 [19:40<02:11,  1.31s/it, v_num=0, train/loss_simple_step=0.547, train/loss_vlb_step=0.00357, train/loss_step=0.547, global_step=491.0, lr_abs=4.91e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7: 100% 1000/1000 [20:17<00:00,  1.22s/it, v_num=0, train/loss_simple_step=0.547, train/loss_vlb_step=0.00357, train/loss_step=0.547, global_step=491.0, lr_abs=4.91e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7: 100% 1000/1000 [20:17<00:00,  1.22s/it, v_num=0, train/loss_simple_step=0.566, train/loss_vlb_step=0.00366, train/loss_step=0.566, global_step=497.0, lr_abs=4.97e-6, train/loss_simple_epoch=0.666, train/loss_vlb_epoch=0.00651, train/loss_epoch=0.666][1,0]<stdout>:Epoch 7: 100% 1000/1000 [21:10<00:00,  1.27s/it, v_num=0, train/loss_simple_step=0.566, train/loss_vlb_step=0.00366, train/loss_step=0.566, global_step=497.0, lr_abs=4.97e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 7:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.566, train/loss_vlb_step=0.00366, train/loss_step=0.566, global_step=497.0, lr_abs=4.97e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567]           [1,0]<stdout>:Epoch 8:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.566, train/loss_vlb_step=0.00366, train/loss_step=0.566, global_step=497.0, lr_abs=4.97e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  10% 100/1000 [00:40<06:05,  2.46it/s, v_num=0, train/loss_simple_step=0.566, train/loss_vlb_step=0.00366, train/loss_step=0.566, global_step=497.0, lr_abs=4.97e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  10% 100/1000 [00:40<06:06,  2.46it/s, v_num=0, train/loss_simple_step=0.461, train/loss_vlb_step=0.00181, train/loss_step=0.461, global_step=504.0, lr_abs=5.04e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  20% 200/1000 [01:19<05:16,  2.53it/s, v_num=0, train/loss_simple_step=0.461, train/loss_vlb_step=0.00181, train/loss_step=0.461, global_step=504.0[1,0]<stderr>:Average Epoch time: 833.27 seconds
[1,0]<stderr>:Average Peak memory 30403.20 MiB
[1,0]<stderr>:Epoch 8, global step 567: 'val/loss_simple_ema' was not in top 3
[1,2]<stderr>:/usr/local/lib/python3.10/dist-packages/PIL/TiffImagePlugin.py:868: UserWarning: Truncated File Read
[1,2]<stderr>:  warnings.warn(str(msg))
[1,0]<stderr>:Average Epoch time: 1351.68 seconds
[1,0]<stderr>:Average Peak memory 30940.60 MiB
[1,0]<stderr>:Epoch 9, global step 630: 'val/loss_simple_ema' was not in top 3
[1,0]<stderr>:`Trainer.fit` stopped: `max_epochs=10` reached.
, lr_abs=5.04e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  20% 200/1000 [01:19<05:16,  2.53it/s, v_num=0, train/loss_simple_step=0.486, train/loss_vlb_step=0.00193, train/loss_step=0.486, global_step=510.0, lr_abs=5.1e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567] [1,0]<stdout>:Epoch 8:  30% 300/1000 [01:58<04:35,  2.54it/s, v_num=0, train/loss_simple_step=0.486, train/loss_vlb_step=0.00193, train/loss_step=0.486, global_step=510.0, lr_abs=5.1e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  30% 300/1000 [01:58<04:35,  2.54it/s, v_num=0, train/loss_simple_step=0.477, train/loss_vlb_step=0.00203, train/loss_step=0.477, global_step=516.0, lr_abs=5.16e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  40% 400/1000 [04:21<06:32,  1.53it/s, v_num=0, train/loss_simple_step=0.477, train/loss_vlb_step=0.00203, train/loss_step=0.477, global_step=516.0, lr_abs=5.16e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  40% 400/1000 [04:21<06:32,  1.53it/s, v_num=0, train/loss_simple_step=0.496, train/loss_vlb_step=0.00208, train/loss_step=0.496, global_step=522.0, lr_abs=5.22e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  50% 500/1000 [05:34<05:34,  1.49it/s, v_num=0, train/loss_simple_step=0.496, train/loss_vlb_step=0.00208, train/loss_step=0.496, global_step=522.0, lr_abs=5.22e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  50% 500/1000 [05:34<05:34,  1.49it/s, v_num=0, train/loss_simple_step=0.420, train/loss_vlb_step=0.00179, train/loss_step=0.420, global_step=529.0, lr_abs=5.29e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  60% 600/1000 [06:14<04:09,  1.60it/s, v_num=0, train/loss_simple_step=0.420, train/loss_vlb_step=0.00179, train/loss_step=0.420, global_step=529.0, lr_abs=5.29e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  60% 600/1000 [06:14<04:09,  1.60it/s, v_num=0, train/loss_simple_step=0.479, train/loss_vlb_step=0.00203, train/loss_step=0.479, global_step=535.0, lr_abs=5.35e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  70% 700/1000 [08:21<03:35,  1.40it/s, v_num=0, train/loss_simple_step=0.479, train/loss_vlb_step=0.00203, train/loss_step=0.479, global_step=535.0, lr_abs=5.35e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  70% 700/1000 [08:21<03:35,  1.39it/s, v_num=0, train/loss_simple_step=0.492, train/loss_vlb_step=0.00226, train/loss_step=0.492, global_step=541.0, lr_abs=5.41e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  80% 800/1000 [08:59<02:14,  1.48it/s, v_num=0, train/loss_simple_step=0.492, train/loss_vlb_step=0.00226, train/loss_step=0.492, global_step=541.0, lr_abs=5.41e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  80% 800/1000 [08:59<02:14,  1.48it/s, v_num=0, train/loss_simple_step=0.389, train/loss_vlb_step=0.00166, train/loss_step=0.389, global_step=547.0, lr_abs=5.47e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  90% 900/1000 [11:45<01:18,  1.28it/s, v_num=0, train/loss_simple_step=0.389, train/loss_vlb_step=0.00166, train/loss_step=0.389, global_step=547.0, lr_abs=5.47e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8:  90% 900/1000 [11:45<01:18,  1.28it/s, v_num=0, train/loss_simple_step=0.473, train/loss_vlb_step=0.0022, train/loss_step=0.473, global_step=554.0, lr_abs=5.54e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567] [1,0]<stdout>:Epoch 8: 100% 1000/1000 [13:52<00:00,  1.20it/s, v_num=0, train/loss_simple_step=0.473, train/loss_vlb_step=0.0022, train/loss_step=0.473, global_step=554.0, lr_abs=5.54e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8: 100% 1000/1000 [13:52<00:00,  1.20it/s, v_num=0, train/loss_simple_step=0.410, train/loss_vlb_step=0.00171, train/loss_step=0.410, global_step=560.0, lr_abs=5.6e-6, train/loss_simple_epoch=0.567, train/loss_vlb_epoch=0.00303, train/loss_epoch=0.567][1,0]<stdout>:Epoch 8: 100% 1000/1000 [13:53<00:00,  1.20it/s, v_num=0, train/loss_simple_step=0.410, train/loss_vlb_step=0.00171, train/loss_step=0.410, global_step=560.0, lr_abs=5.6e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 8:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.410, train/loss_vlb_step=0.00171, train/loss_step=0.410, global_step=560.0, lr_abs=5.6e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458]           [1,0]<stdout>:Epoch 9:   0% 0/1000 [00:00<?, ?it/s, v_num=0, train/loss_simple_step=0.410, train/loss_vlb_step=0.00171, train/loss_step=0.410, global_step=560.0, lr_abs=5.6e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  10% 100/1000 [01:57<17:41,  1.18s/it, v_num=0, train/loss_simple_step=0.410, train/loss_vlb_step=0.00171, train/loss_step=0.410, global_step=560.0, lr_abs=5.6e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  10% 100/1000 [01:57<17:41,  1.18s/it, v_num=0, train/loss_simple_step=0.408, train/loss_vlb_step=0.00169, train/loss_step=0.408, global_step=567.0, lr_abs=5.67e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  20% 200/1000 [03:54<15:38,  1.17s/it, v_num=0, train/loss_simple_step=0.408, train/loss_vlb_step=0.00169, train/loss_step=0.408, global_step=567.0, lr_abs=5.67e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  20% 200/1000 [03:54<15:38,  1.17s/it, v_num=0, train/loss_simple_step=0.492, train/loss_vlb_step=0.00467, train/loss_step=0.492, global_step=573.0, lr_abs=5.73e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  30% 300/1000 [05:59<13:59,  1.20s/it, v_num=0, train/loss_simple_step=0.492, train/loss_vlb_step=0.00467, train/loss_step=0.492, global_step=573.0, lr_abs=5.73e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  30% 300/1000 [05:59<13:59,  1.20s/it, v_num=0, train/loss_simple_step=0.424, train/loss_vlb_step=0.00226, train/loss_step=0.424, global_step=579.0, lr_abs=5.79e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  40% 400/1000 [08:09<12:14,  1.22s/it, v_num=0, train/loss_simple_step=0.424, train/loss_vlb_step=0.00226, train/loss_step=0.424, global_step=579.0, lr_abs=5.79e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  40% 400/1000 [08:09<12:14,  1.22s/it, v_num=0, train/loss_simple_step=0.414, train/loss_vlb_step=0.00166, train/loss_step=0.414, global_step=585.0, lr_abs=5.85e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  50% 500/1000 [09:56<09:56,  1.19s/it, v_num=0, train/loss_simple_step=0.414, train/loss_vlb_step=0.00166, train/loss_step=0.414, global_step=585.0, lr_abs=5.85e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  50% 500/1000 [09:56<09:56,  1.19s/it, v_num=0, train/loss_simple_step=0.432, train/loss_vlb_step=0.0024, train/loss_step=0.432, global_step=592.0, lr_abs=5.92e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458] [1,0]<stdout>:Epoch 9:  60% 600/1000 [13:10<08:47,  1.32s/it, v_num=0, train/loss_simple_step=0.432, train/loss_vlb_step=0.0024, train/loss_step=0.432, global_step=592.0, lr_abs=5.92e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  60% 600/1000 [13:10<08:47,  1.32s/it, v_num=0, train/loss_simple_step=0.338, train/loss_vlb_step=0.00143, train/loss_step=0.338, global_step=598.0, lr_abs=5.98e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  70% 700/1000 [14:43<06:18,  1.26s/it, v_num=0, train/loss_simple_step=0.338, train/loss_vlb_step=0.00143, train/loss_step=0.338, global_step=598.0, lr_abs=5.98e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  70% 700/1000 [14:43<06:18,  1.26s/it, v_num=0, train/loss_simple_step=0.346, train/loss_vlb_step=0.00138, train/loss_step=0.346, global_step=604.0, lr_abs=6.04e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  80% 800/1000 [17:42<04:25,  1.33s/it, v_num=0, train/loss_simple_step=0.346, train/loss_vlb_step=0.00138, train/loss_step=0.346, global_step=604.0, lr_abs=6.04e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  80% 800/1000 [17:42<04:25,  1.33s/it, v_num=0, train/loss_simple_step=0.363, train/loss_vlb_step=0.00151, train/loss_step=0.363, global_step=610.0, lr_abs=6.1e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458] [1,0]<stdout>:Epoch 9:  90% 900/1000 [18:55<02:06,  1.26s/it, v_num=0, train/loss_simple_step=0.363, train/loss_vlb_step=0.00151, train/loss_step=0.363, global_step=610.0, lr_abs=6.1e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9:  90% 900/1000 [18:55<02:06,  1.26s/it, v_num=0, train/loss_simple_step=0.404, train/loss_vlb_step=0.00273, train/loss_step=0.404, global_step=617.0, lr_abs=6.17e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9: 100% 1000/1000 [22:31<00:00,  1.35s/it, v_num=0, train/loss_simple_step=0.404, train/loss_vlb_step=0.00273, train/loss_step=0.404, global_step=617.0, lr_abs=6.17e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9: 100% 1000/1000 [22:31<00:00,  1.35s/it, v_num=0, train/loss_simple_step=0.375, train/loss_vlb_step=0.00549, train/loss_step=0.375, global_step=623.0, lr_abs=6.23e-6, train/loss_simple_epoch=0.458, train/loss_vlb_epoch=0.00195, train/loss_epoch=0.458][1,0]<stdout>:Epoch 9: 100% 1000/1000 [22:31<00:00,  1.35s/it, v_num=0, train/loss_simple_step=0.375, train/loss_vlb_step=0.00549, train/loss_step=0.375, global_step=623.0, lr_abs=6.23e-6, train/loss_simple_epoch=0.400, train/loss_vlb_epoch=0.00252, train/loss_epoch=0.400][1,0]<stdout>:Epoch 9: 100% 1000/1000 [23:37<00:00,  1.42s/it, v_num=0, train/loss_simple_step=0.375, train/loss_vlb_step=0.00549, train/loss_step=0.375, global_step=623.0, lr_abs=6.23e-6, train/loss_simple_epoch=0.400, train/loss_vlb_epoch=0.00252, train/loss_epoch=0.400][1,0]<stdout>:
[1,0]<stdout>:
