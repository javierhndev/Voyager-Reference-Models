apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: yourusername-llama-16cards
  namespace: default
spec:
  slotsPerWorker: 8
  runPolicy:
    cleanPodPolicy: Running
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        spec:
          volumes:
            - name: mydir
              hostPath:
                path: /home/yourusername/models/voyager/llama/1.13
                type: Directory
            - name: scratch
              emptyDir: {}
            - name: output
              hostPath:
                path: /voyager/ceph/users/yourusername/results/llama/1.13
                type: Directory
            - name: dataset
              hostPath:
                path: /voyager/ceph/users/czhao/datasets/llama/redpajama_tokenized
                type: Directory
          hostIPC: true
          containers:
            - image: vault.habana.ai/gaudi-docker/1.13.0/ubuntu20.04/habanalabs/pytorch-installer-2.1.0:latest
              name: llama-launcher
              volumeMounts:
                - name: mydir
                  mountPath: /mydir
                - name: scratch
                  mountPath: /scratch
                - name: output
                  mountPath: /output
                - name: dataset
                  mountPath: /dataset
                  #env:
                  #- name: results
                  #value: "/ceph/results/llama"
              command: ["/bin/bash", "-c"]
              args:
                - >-
                  declare -xr NUM_NODES=2;
                  declare -xr SYNAPSE_AI_VER=1.13.0;
                  declare -xr RUN_PATH=/mydir;
                  declare -xr MODEL_ROOT=/scratch/Model-References;
                  declare -xr MODEL_PATH=$MODEL_ROOT/PyTorch/nlp/DeepSpeedExamples/Megatron-DeepSpeed;

                  declare -xr HL_DATA_DIR_ROOT=/dataset;
                  declare -xr HL_LLAMA_VER=7;
                  declare -xr ZERO_STAGE=0;
                  declare -xr HL_EXIT_INTERVAL=100;

                  declare -xr HL_DP=2;
                  declare -xr HL_TP=8;
                  declare -xr HL_PP=1;

                  sleep 10s;
                  $RUN_PATH/setup.sh;

                  mpirun  --npernode 1 \
                    -np $NUM_NODES \
                    --tag-output \
                    --allow-run-as-root \
                    --prefix $MPI_ROOT \
                    -x MODEL_PATH \
                    -x SYNAPSE_AI_VER \
                    -x RUN_PATH \
                    $RUN_PATH/setup.sh;

                  $RUN_PATH/run_llama.sh;
                  #sleep infinity;


    Worker:
      replicas: 2
      template:
        spec:
          volumes:
            - name: mydir
              hostPath:
                path: /home/yourusername/models/voyager/llama/1.13
                type: Directory
            - name: scratch
              emptyDir: {}
            - name: output
              hostPath:
                path: /voyager/ceph/users/yourusername/results/llama/1.13
                type: Directory
            - name: dataset
              hostPath:
                path: /voyager/ceph/users/czhao/datasets/llama/redpajama_tokenized
                type: Directory
          hostIPC: true
          containers:
            - image: vault.habana.ai/gaudi-docker/1.13.0/ubuntu20.04/habanalabs/pytorch-installer-2.1.0:latest
              name: llama-worker
              imagePullPolicy: Always
              resources:
                limits:
                  habana.ai/gaudi: 8
                  cpu: 95
                  memory: 409Gi
                  hugepages-2Mi: 95000Mi
                requests:
                  habana.ai/gaudi: 8
                  cpu: 95
                  memory: 409Gi
                  hugepages-2Mi: 95000Mi
              volumeMounts:
                - name: mydir
                  mountPath: /mydir
                - name: scratch
                  mountPath: /scratch
                - name: output
                  mountPath: /output
                - name: dataset
                  mountPath: /dataset
 
