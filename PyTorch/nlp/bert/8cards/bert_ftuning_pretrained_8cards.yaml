apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: yourusername-bert-8cards
  namespace: default
spec:
  slotsPerWorker: 8
  runPolicy:
    cleanPodPolicy: Running
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        spec:
          volumes:
            - name: mydir
              hostPath:
                path: /home/yourusername/models/voyager/bert_pytorch/1.13.0/multicard
                type: Directory
            - name: scratch
              emptyDir: {}
            - name: dataset
              hostPath:
                path: /voyager/ceph/users/yourusername/datasets/bert/pytorch
                type: Directory
            - name: output
              hostPath:
                path: /voyager/ceph/users/yourusername/results/bert_pytorch/8cards
                type: Directory
            - name: dllog
              hostPath:
                path: /voyager/ceph/users/yourusername/tmp/log_directory
                type: Directory
          hostIPC: true
          containers:
            - image: vault.habana.ai/gaudi-docker/1.13.0/ubuntu22.04/habanalabs/pytorch-installer-2.1.0:latest
              name: bert-launcher
              volumeMounts:
                - name: mydir
                  mountPath: /mydir
                - name: scratch
                  mountPath: /scratch
                - name: dataset
                  mountPath: /dataset
                - name: output
                  mountPath: /output
                - name: dllog
                  mountPath: /dllog
                  #env:
                  #- name: dataset
                  #value: "/ceph/datasets/bert/pytorch"
                  #- name: output
                  #value: "/ceph/results/bert_pytorch/8cards"
                  #- name: dllog
                  #value: "/ceph/tmp/log_directory"
              command: ["/bin/bash", "-c"]
              args:
                - >-
                  declare -xr NUM_NODES=1;
                  declare -xr NGPU_PER_NODE=8;
                  declare -xr N_CARDS=$((NUM_NODES*NGPU_PER_NODE));

                  declare -xr SYNAPSE_AI_VER=1.13.0;
                  declare -xr RUN_PATH=/mydir;
                  declare -xr MODEL_PATH=/scratch/Model-References/PyTorch/nlp/bert;

                  declare -xr PYTHONPATH=$PYTHONPATH:/scratch/Model-References;

                  HOSTSFILE=${HOSTSFILE:-$OMPI_MCA_orte_default_hostfile};
                  declare -xr MASTER_ADDR=$(head -n 1 $HOSTSFILE | sed -n s/[[:space:]]slots.*//p);
                  declare -xr MASTER_PORT=${MASTER_PORT:-15566};

                  echo $MASTER_ADDR;
                  echo $MASTER_PORT;

                  sleep 15s;

                  mpirun  --npernode 1 \
                    --tag-output \
                    --allow-run-as-root \
                    --prefix $MPI_ROOT \
                    -x MODEL_PATH \
                    -x SYNAPSE_AI_VER \
                    $RUN_PATH/setup.sh;

                  declare -xr CMD="python3 $MODEL_PATH/run_squad.py \
                                   --do_train \
                                   --bert_model=bert-large-uncased \
                                   --config_file=$MODEL_PATH/bert_config.json \
                                   --use_habana \
                                   --autocast \
                                   --use_fused_adam \
                                   --do_lower_case \
                                   --output_dir=/output \
                                   --json-summary=/dllog/dllogger.json \
                                   --train_batch_size=24 \
                                   --predict_batch_size=8 \
                                   --seed=1 \
                                   --max_seq_length=384 \
                                   --doc_stride=128 \
                                   --max_steps=-1  \
                                   --learning_rate=3e-5 \
                                   --num_train_epochs=2 \
                                   --init_checkpoint=/dataset/pretrained-habana/1.13/ckpt_7038.pt \
                                   --vocab_file=/dataset/uncased_L-24_H-1024_A-16/vocab.txt \
                                   --train_file=/dataset/squad/v1.1/train-v1.1.json \
                                   --skip_cache \
                                   --do_predict  \
                                   --predict_file=/dataset/squad/v1.1/dev-v1.1.json \
                                   --do_eval \
                                   --eval_script=/dataset/squad/v1.1/evaluate-v1.1.py \
                                   --log_freq 20 ";
                  
                  
                 
                  

                  mpirun -np ${N_CARDS} \
                    --allow-run-as-root \
                    --bind-to core \
                    --map-by ppr:4:socket:PE=6 \
                    -rank-by core --report-bindings \
                    --tag-output \
                    --merge-stderr-to-stdout --prefix $MPI_ROOT \
                    -x MASTER_ADDR=$MASTER_ADDR \
                    -x MASTER_PORT=$MASTER_PORT \
                    -x PYTHONPATH \
                    $CMD;

                    #sleep infinity;

    Worker:
      replicas: 1
      template:
        spec:
          volumes:
            - name: mydir
              hostPath:
                path: /home/yourusername/models/voyager/bert_pytorch/1.13.0/multicard
                type: Directory
            - name: scratch
              emptyDir: {}
            - name: dataset
              hostPath:
                path: /voyager/ceph/users/yourusername/datasets/bert/pytorch
                type: Directory
            - name: output
              hostPath:
                path: /voyager/ceph/users/yourusername/results/bert_pytorch/8cards
                type: Directory
            - name: dllog
              hostPath:
                path: /voyager/ceph/users/yourusername/tmp/log_directory
                type: Directory
          hostIPC: true
          containers:
            - image: vault.habana.ai/gaudi-docker/1.13.0/ubuntu22.04/habanalabs/pytorch-installer-2.1.0:latest
              name: bert-worker
              imagePullPolicy: Always
              resources:
                limits:
                  habana.ai/gaudi: 8
                  cpu: 95
                  memory: 409Gi
                  hugepages-2Mi: 95000Mi
                requests:
                  habana.ai/gaudi: 8
                  cpu: 95
                  memory: 409Gi
                  hugepages-2Mi: 95000Mi
              volumeMounts:
                - name: mydir
                  mountPath: /mydir
                - name: scratch
                  mountPath: /scratch
                - name: dataset
                  mountPath: /dataset
                - name: output
                  mountPath: /output
                - name: dllog
                  mountPath: /dllog

