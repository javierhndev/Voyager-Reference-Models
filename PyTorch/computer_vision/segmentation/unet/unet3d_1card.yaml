apiVersion: v1
kind: Pod
metadata:
  name: unet-3d
spec:
  restartPolicy: Never
  volumes:
    - name: scratch
      emptyDir: {}
    - name: dataset
      hostPath:
        path: /voyager/ceph/users/yourusername/datasets/BraTS/pytorch/unet/01_3d
        type: Directory
    - name: output
      hostPath:
        path: /voyager/ceph/users/yourusername/results/unet_pytorch/3d/fold_0
        type: Directory
  containers:
    - name: gaudi-container
      image: vault.habana.ai/gaudi-docker/1.15.1/ubuntu22.04/habanalabs/pytorch-installer-2.2.0:latest
      volumeMounts:
        - mountPath: /scratch
          name: scratch
        - mountPath: /dataset
          name: dataset 
        - mountPath: /output
          name: output 
      resources:
         limits:
           memory: 32G
           cpu: 12
           habana.ai/gaudi: 1
           hugepages-2Mi: 95000Mi
         requests:
           memory: 32G
           cpu: 12
           habana.ai/gaudi: 1
           hugepages-2Mi: 95000Mi
           #env:
           #- name: dataset
           #value: "/ceph/datasets/BraTS/pytorch/unet/01_2d"
           #- name: output
           #value: "/ceph/results/unet_pytorch/2d/fold_0"
      command: ["/bin/sh","-c"]
      args:
        - hl-smi;
          export HOME=/scratch/tmp;
          mkdir -p /scratch/tmp;
          cd /scratch;
          git clone -b 1.15.1 https://github.com/HabanaAI/Model-References;
          export PYTHONPATH=/scratch/Model-References:$PYTHONPATH;
          export PATH=/scratch/tmp/.local/bin:$PATH;
          cd Model-References/PyTorch/computer_vision/segmentation/Unet;
          pip install -r ./requirements.txt;
          python3 -u  main.py --results /output --task 01 --logname res_log --fold 0 --hpus 1 --gpus 0 --data /dataset --seed 1 --num_workers 8 --affinity disabled --norm instance --dim 3 --optimizer fusedadamw  --exec_mode train --learning_rate 0.001 --autocast --deep_supervision --batch_size 2 --val_batch_size 2 ; 
