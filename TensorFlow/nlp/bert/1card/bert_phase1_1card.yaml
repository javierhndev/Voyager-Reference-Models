apiVersion: v1
kind: Pod
metadata:
  name: bert1
spec:
  restartPolicy: Never
  volumes:
    - name: scratch
      emptyDir: {}
    - name: dataset
      hostPath:
        path: /voyager/ceph/users/yourusername/datasets/bert/books_wiki_en_corpus/tfrecord
        type: Directory
    - name: output
      hostPath:
        path: /voyager/ceph/users/yourusername/results/bert_tf
        type: Directory
    - name: pretrained
      hostPath:
        path: /voyager/ceph/users/yourusername/datasets/bert/pretrained/wwm_uncased_L-24_H-1024_A-16
        type: Directory
  containers:
    - name: gaudi-container
      image: vault.habana.ai/gaudi-docker/1.13.0/ubuntu22.04/habanalabs/tensorflow-installer-tf-cpu-2.13.1
      volumeMounts:
        - mountPath: /scratch
          name: scratch
        - mountPath: /dataset
          name: dataset 
        - mountPath: /output
          name: output 
        - mountPath: /pretrained
          name: pretrained 
      resources:
        limits:
          memory: 64G
          cpu: 6
          habana.ai/gaudi: 1
          hugepages-2Mi: 95000Mi
        requests:
          memory: 64G
          cpu: 6
          habana.ai/gaudi: 1
          hugepages-2Mi: 95000Mi
          #env:
          #- name: input
          #value: "/ceph/datasets/bert/books_wiki_en_corpus/tfrecord/seq_len_128/wikicorpus_en/training"
          #- name: eval
          #value: "/ceph/datasets/bert/books_wiki_en_corpus/tfrecord/seq_len_128/wikicorpus_en/test"
          #- name: pretrain
          #value: "/ceph/datasets/bert/pretrained/wwm_uncased_L-24_H-1024_A-16/bert_config.json"
      command: ["/bin/sh","-c"]
      args:
         - >-
             hl-smi;
             export HOME=/scratch/tmp;
             cd /scratch;
             mkdir -p tmp;
             git clone -b 1.13.0 https://github.com/HabanaAI/Model-References;
             cd Model-References/TensorFlow/nlp/bert/;
             export PYTHONPATH=/scratch/Model-References:/scratch/Model-References/TensorFlow/nlp/bert/:$PYTHONPATH;
             export PATH=$PATH:/scratch/tmp/.local/bin;
             TF_BF16_CONVERSION=/scratch/Model-References/TensorFlow/nlp/bert/bf16_config/bert.json;
             pip install -r requirements.txt;
             python3 run_pretraining.py \
                     --input_files_dir=/dataset/seq_len_128/wikicorpus_en/training  \
                     --init_checkpoint= \
                     --eval_files_dir=/dataset/seq_len_128/wikicorpus_en/test \
                     --output_dir=/output/phase_1 \
                     --bert_config_file=/pretrained/bert_config.json \
                     --do_train=True \
                     --do_eval=False \
                     --train_batch_size=64 \
                     --eval_batch_size=8 \
                     --max_seq_length=128 \
                     --max_predictions_per_seq=20 \
                     --num_train_steps=7038 \
                     --num_accumulation_steps=1024 \
                     --num_warmup_steps=2000 \
                     --save_checkpoints_steps=100  \
                     --learning_rate=0.006 \
                     --noamp \
                     --nouse_xla \
                     --allreduce_post_accumulation=True \
                     --dllog_path=/output/phase_1/bert_dllog.json  \
                     --enable_scoped_allocator=False \
                     --resume=False;
 
